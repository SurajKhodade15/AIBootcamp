{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6060acc8",
   "metadata": {},
   "source": [
    "Here‚Äôs the **LSTM RNN Architecture** explanation in a **Python Notebook‚Äìfriendly Markdown format** so it will display cleanly in Jupyter without breaking formatting.\n",
    "\n",
    "---\n",
    "\n",
    "## **LSTM RNN Architecture**\n",
    "\n",
    "| **Component**         | **Description**                                                                                                                                                                                                               |\n",
    "| --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Cell State**        | The \"memory\" of the network that carries long-term information across time steps with minimal changes. It runs through the entire sequence like a conveyor belt, enabling the model to remember patterns over long sequences. |\n",
    "| **Forget Gate**       | Decides which information to discard from the cell state. It takes the previous hidden state ($h_{t-1}$) and current input ($x_t$) and outputs values between 0 and 1 (via sigmoid) to remove irrelevant data.                |\n",
    "| **Input Gate**        | Decides which new information to add to the cell state. It uses a sigmoid function to control what values will be updated and a tanh function to create candidate values ($\\tilde{C}_t$) to be added.                         |\n",
    "| **Cell State Update** | Combines the forget gate‚Äôs filtered old cell state with the input gate‚Äôs candidate values to form the new cell state $C_t$.                                                                                                   |\n",
    "| **Output Gate**       | Decides what the next hidden state ($h_t$) should be. It applies a sigmoid function to control output, then multiplies by the tanh of the updated cell state.                                                                 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "For a time step $t$:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad \\text{(Forget Gate)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad \\text{(Input Gate)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\quad \\text{(Candidate Cell State)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = f_t \\ast C_{t-1} + i_t \\ast \\tilde{C}_t \\quad \\text{(Cell State Update)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad \\text{(Output Gate)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = o_t \\ast \\tanh(C_t) \\quad \\text{(Hidden State Update)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **LSTM Cell Diagram**\n",
    "\n",
    "*(For notebook, you can use this simple ASCII diagram or insert an image.)*\n",
    "\n",
    "```\n",
    "          +-------------------+\n",
    "   xt --->|                   |\n",
    "          |   Forget Gate     |\n",
    " ht-1 --->|                   |---- f_t\n",
    "          +-------------------+\n",
    "                  |\n",
    "                  v\n",
    "   xt --->+-------------------+\n",
    "          |                   |\n",
    "          |   Input Gate      |\n",
    " ht-1 --->|                   |---- i_t, C~_t\n",
    "          +-------------------+\n",
    "                  |\n",
    "                  v\n",
    "          +-------------------+\n",
    "          | Cell State Update |\n",
    "          +-------------------+\n",
    "                  |\n",
    "                  v\n",
    "   xt --->+-------------------+\n",
    "          |                   |\n",
    "          |   Output Gate     |\n",
    " ht-1 --->|                   |---- o_t, h_t\n",
    "          +-------------------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Advantages Over Simple RNN**\n",
    "\n",
    "| Feature                        | Simple RNN | LSTM                     |\n",
    "| ------------------------------ | ---------- | ------------------------ |\n",
    "| Handles Long-Term Dependencies | ‚ùå          | ‚úÖ                        |\n",
    "| Vanishing Gradient Problem     | Severe     | Greatly Reduced          |\n",
    "| Complexity                     | Low        | Higher                   |\n",
    "| Training Time                  | Faster     | Slower but More Accurate |\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can now prepare the **LSTM Forward Propagation with Time** explanation in the **same notebook-friendly format** so your notes stay consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e1087",
   "metadata": {},
   "source": [
    "# üìò Introduction to LSTM (Long Short-Term Memory)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What is LSTM?\n",
    "\n",
    "Long Short-Term Memory (LSTM) is an advanced type of Recurrent Neural Network (RNN) designed to **learn long-term dependencies** in sequential data. It was introduced by **Hochreiter & Schmidhuber (1997)** to address issues like **vanishing gradients** faced by traditional RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Why LSTM?\n",
    "\n",
    "- Traditional RNNs struggle to remember long-term dependencies.\n",
    "- LSTM incorporates **memory cells** and **gating mechanisms** to control the flow of information.\n",
    "- Capable of learning over **long sequences** without losing context.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è LSTM Architecture Components\n",
    "\n",
    "1. **Forget Gate** \\( f_t \\):  \n",
    "   Decides what information to discard from the cell state.\n",
    "   \\[\n",
    "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "   \\]\n",
    "\n",
    "2. **Input Gate** \\( i_t \\):  \n",
    "   Decides what new information to store in the cell state.\n",
    "   \\[\n",
    "   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "   \\]\n",
    "\n",
    "3. **Candidate Cell State** \\( \\tilde{C}_t \\):  \n",
    "   Proposes new candidate values.\n",
    "   \\[\n",
    "   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "   \\]\n",
    "\n",
    "4. **Cell State Update** \\( C_t \\):  \n",
    "   Updates the cell state with retained and new information.\n",
    "   \\[\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "   \\]\n",
    "\n",
    "5. **Output Gate** \\( o_t \\):  \n",
    "   Determines what part of the cell state to output.\n",
    "   \\[\n",
    "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "   \\]\n",
    "\n",
    "6. **Hidden State Update** \\( h_t \\):  \n",
    "   Final output of the LSTM block.\n",
    "   \\[\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   \\]\n",
    "\n",
    "Where:\n",
    "- \\( \\sigma \\): Sigmoid activation function  \n",
    "- \\( \\odot \\): Element-wise multiplication  \n",
    "- \\( x_t \\): Input at time step t  \n",
    "- \\( h_{t-1} \\): Previous hidden state  \n",
    "- \\( C_t \\): Cell state\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Features\n",
    "\n",
    "- Remembers **long-term dependencies**\n",
    "- **Reduces vanishing gradient problem**\n",
    "- Uses **gating mechanism** to manage memory\n",
    "- Works well for **sequence prediction tasks**\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Use Cases\n",
    "\n",
    "- Language Modeling and Text Generation\n",
    "- Machine Translation\n",
    "- Time Series Forecasting\n",
    "- Sentiment Analysis\n",
    "- Speech Recognition\n",
    "- Video Classification\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Keras Code Example\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(10, 50)))  # 10 time steps, 50 features\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774269aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
