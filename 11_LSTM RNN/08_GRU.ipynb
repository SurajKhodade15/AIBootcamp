{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1827796a",
   "metadata": {},
   "source": [
    "Certainly. Below is a comprehensive overview of **GRU (Gated Recurrent Unit)**, formatted for your Jupyter Notebook markdown:\n",
    "\n",
    "---\n",
    "\n",
    "## GRU (Gated Recurrent Unit)\n",
    "\n",
    "| Aspect                                                                 | Details                                                                                                                                                                                                                                                                                                                                                      |\n",
    "| ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Definition**                                                         | GRU is a streamlined variant of LSTM that combines the forget and input gates into a single **update gate**, and merges the cell state and hidden state, resulting in a simpler architecture.                                                                                                                                                                |\n",
    "| **Invented By**                                                        | Cho et al., 2014                                                                                                                                                                                                                                                                                                                                             |\n",
    "| **Motivation**                                                         | Designed to reduce model complexity and computational cost while maintaining the ability to capture long-term dependencies in sequence data.                                                                                                                                                                                                                 |\n",
    "| **Architecture Components**                                            | - **Update Gate $z_t$**: Controls how much of the previous hidden state to keep.<br>- **Reset Gate $r_t$**: Decides how to combine new input with past memory.<br>- **Candidate Activation $\\tilde{h}_t$**: New memory content created with reset gate applied.<br>- **Hidden State $h_t$**: Final output combining previous state and candidate activation. |\n",
    "| **Mathematical Formulas**                                              | \\[                                                                                                                                                                                                                                                                                                                                                           |\n",
    "| \\begin{aligned}                                                        |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| z\\_t &= \\sigma(W\\_z \\cdot \\[h\\_{t-1}, x\\_t] + b\\_z) \\\\                 |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| r\\_t &= \\sigma(W\\_r \\cdot \\[h\\_{t-1}, x\\_t] + b\\_r) \\\\                 |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| \\tilde{h}*t &= \\tanh(W\\_h \\cdot \\[r\\_t \\odot h*{t-1}, x\\_t] + b\\_h) \\\\ |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| h\\_t &= (1 - z\\_t) \\odot h\\_{t-1} + z\\_t \\odot \\tilde{h}\\_t            |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| \\end{aligned}                                                          |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| ]                                                                      |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| **Key Characteristics**                                                | - Fewer parameters than LSTM (no separate cell state).<br>- Faster to train due to simpler structure.<br>- Maintains capability to capture long-term dependencies.<br>- Empirically comparable performance to LSTM on many tasks.                                                                                                                            |\n",
    "| **Advantages**                                                         | - Computationally efficient.<br>- Suitable for smaller datasets or real-time applications.<br>- Less prone to overfitting due to fewer parameters.                                                                                                                                                                                                           |\n",
    "| **Limitations**                                                        | - May perform slightly worse than LSTM on some very complex sequence tasks.<br>- Lack of explicit memory cell may reduce representational flexibility.                                                                                                                                                                                                       |\n",
    "| **Use Cases**                                                          | - Speech recognition<br>- Natural language processing<br>- Time series prediction<br>- Real-time sequence modeling                                                                                                                                                                                                                                           |\n",
    "| **Python Example (Keras)**                                             | \\`\\`\\`python                                                                                                                                                                                                                                                                                                                                                 |\n",
    "| from tensorflow\\.keras.models import Sequential                        |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| from tensorflow\\.keras.layers import GRU, Dense                        |                                                                                                                                                                                                                                                                                                                                                              |\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(128, input\\_shape=(10, 50)))  # 10 time steps, 50 features\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary\\_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "```|\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a7632",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
