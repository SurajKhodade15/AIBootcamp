{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f94a0fa",
   "metadata": {},
   "source": [
    "\n",
    "## Forget Gate in LSTM\n",
    "\n",
    "| Aspect                   | Details                                                                                                                                                                                                                                                                |\n",
    "| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**           | The forget gate in an LSTM cell determines which information from the previous cell state should be discarded (forgotten) and which should be retained.                                                                                                                |\n",
    "| **Purpose**              | Controls the flow of past information by applying a sigmoid activation function, producing values between 0 (completely forget) and 1 (completely retain).                                                                                                             |\n",
    "| **Mathematical Formula** | $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$ <br> - $f_t$: Forget gate output <br> - $\\sigma$: Sigmoid activation <br> - $W_f$: Weight matrix for forget gate <br> - $h_{t-1}$: Previous hidden state <br> - $x_t$: Current input <br> - $b_f$: Bias for forget gate |\n",
    "| **Working**              | 1. Takes the previous hidden state ($h_{t-1}$) and current input ($x_t$). <br> 2. Passes them through a sigmoid activation. <br> 3. Produces a vector of values between 0 and 1 indicating how much of each element in the previous cell state $C_{t-1}$ to keep.      |\n",
    "| **Example Scenario**     | In a language model, if the previous context is irrelevant to the next word prediction, the forget gate will output values closer to 0 for that context, effectively removing it from memory.                                                                          |\n",
    "| **Use Cases**            | - Machine translation <br> - Text summarization <br> - Speech recognition <br> - Time-series forecasting                                                                                                                                                               |\n",
    "| **Interview Q\\&A**       | **Q:** Why does the forget gate use sigmoid activation instead of ReLU or tanh? <br> **A:** Sigmoid outputs a value between 0 and 1, making it ideal for \"retention ratio\" control, whereas ReLU and tanh are not bounded in \\[0, 1] for this purpose.                 |\n",
    "\n",
    "---\n",
    "\n",
    "### Python Example â€“ Forget Gate in LSTM\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample input values\n",
    "x_t = np.array([0.5, 0.1])      # Current input\n",
    "h_t_minus_1 = np.array([0.2, 0.4])  # Previous hidden state\n",
    "\n",
    "# Weight and bias initialization (random for example)\n",
    "W_f = np.random.randn(4, 2)   # Weight matrix for forget gate\n",
    "b_f = np.random.randn(2)      # Bias vector\n",
    "\n",
    "# Concatenate h_(t-1) and x_t\n",
    "concat_input = np.concatenate((h_t_minus_1, x_t))\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forget gate output\n",
    "f_t = sigmoid(np.dot(concat_input, W_f) + b_f)\n",
    "\n",
    "print(\"Forget Gate Output:\", f_t)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc01416",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
