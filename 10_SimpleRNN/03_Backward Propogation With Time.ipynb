{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a109ef",
   "metadata": {},
   "source": [
    "Here is the **Backward Propagation Through Time (BPTT) in RNN** with the **code properly formatted**:\n",
    "\n",
    "---\n",
    "\n",
    "### **Backward Propagation Through Time (BPTT) in RNN**\n",
    "\n",
    "| **Aspect**               | **Details**                                                                                                                                                                                                                                                                                                                                                                        |\n",
    "| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**           | Backpropagation Through Time (BPTT) is the training algorithm for RNNs where errors are propagated **backward through each time step** to update network weights. It is an extension of standard backpropagation adapted to sequential data.                                                                                                                                       |\n",
    "| **Concept**              | - The RNN is **unrolled** across all time steps $T$.<br>- Forward propagation calculates hidden states $h_t$ and outputs $y_t$.<br>- The total loss $L$ is computed as the sum of losses across all time steps.<br>- Gradients are computed by applying the chain rule **through time**, moving from the last time step $T$ back to the first.                                     |\n",
    "| **Loss Function**        | For a sequence of length $T$, total loss is:<br> $L = \\sum_{t=1}^{T} L_t(y_t, \\hat{y_t})$                                                                                                                                                                                                                                                                                          |\n",
    "| **Gradient Calculation** | - Gradients at time $t$ depend on both the error at time $t$ and future time steps.<br>- Weight updates are accumulated across all time steps:<br> $\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial W_t}$                                                                                                                                                |\n",
    "| **Equations**            | 1. Output error: $\\delta y_t = y_t - \\hat{y_t}$<br>2. Hidden state error: $\\delta h_t = (W_y^T \\delta y_t + W_h^T \\delta h_{t+1}) \\odot f'(h_t)$<br>3. Weight gradients:<br> - $\\frac{\\partial L}{\\partial W_y} = \\sum_t \\delta y_t h_t^T$<br> - $\\frac{\\partial L}{\\partial W_h} = \\sum_t \\delta h_t h_{t-1}^T$<br> - $\\frac{\\partial L}{\\partial W_x} = \\sum_t \\delta h_t x_t^T$ |\n",
    "| **Key Issues**           | - **Vanishing Gradient:** Gradients shrink exponentially for long sequences.<br>- **Exploding Gradient:** Gradients grow uncontrollably.<br>- **Solutions:** Gradient clipping, LSTM, GRU, residual connections.                                                                                                                                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "### **Python Example: BPTT Implementation (Conceptual)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "T = 5                     # Sequence length\n",
    "hidden_size = 4           # Hidden layer size\n",
    "input_size = 3            # Input feature size\n",
    "output_size = 2           # Output size\n",
    "lr = 0.01                 # Learning rate\n",
    "\n",
    "# Initialize weights and biases\n",
    "W_x = np.random.randn(hidden_size, input_size)\n",
    "W_h = np.random.randn(hidden_size, hidden_size)\n",
    "W_y = np.random.randn(output_size, hidden_size)\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "b_y = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input sequence and target outputs\n",
    "X = [np.random.randn(input_size, 1) for _ in range(T)]\n",
    "Y = [np.random.randn(output_size, 1) for _ in range(T)]\n",
    "\n",
    "# Forward Pass\n",
    "h = [np.zeros((hidden_size, 1))]\n",
    "y = []\n",
    "for t in range(T):\n",
    "    h_t = np.tanh(W_x @ X[t] + W_h @ h[-1] + b_h)   # Hidden state\n",
    "    y_t = W_y @ h_t + b_y                           # Output\n",
    "    h.append(h_t)\n",
    "    y.append(y_t)\n",
    "\n",
    "# Loss gradients at output (dummy gradient)\n",
    "dL_dy = [y[t] - Y[t] for t in range(T)]\n",
    "\n",
    "# Initialize gradients\n",
    "dW_x = np.zeros_like(W_x)\n",
    "dW_h = np.zeros_like(W_h)\n",
    "dW_y = np.zeros_like(W_y)\n",
    "db_h = np.zeros_like(b_h)\n",
    "db_y = np.zeros_like(b_y)\n",
    "dh_next = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Backward Propagation Through Time\n",
    "for t in reversed(range(T)):\n",
    "    # Output layer gradient\n",
    "    dW_y += dL_dy[t] @ h[t+1].T\n",
    "    db_y += dL_dy[t]\n",
    "\n",
    "    # Hidden layer gradient\n",
    "    dh = W_y.T @ dL_dy[t] + dh_next\n",
    "    dtanh = (1 - h[t+1]**2) * dh   # Derivative of tanh\n",
    "\n",
    "    # Weight gradients\n",
    "    dW_x += dtanh @ X[t].T\n",
    "    dW_h += dtanh @ h[t].T\n",
    "    db_h += dtanh\n",
    "\n",
    "    # Propagate error to previous time step\n",
    "    dh_next = W_h.T @ dtanh\n",
    "\n",
    "# Parameter Update (SGD)\n",
    "for param, dparam in zip([W_x, W_h, W_y, b_h, b_y],\n",
    "                         [dW_x, dW_h, dW_y, db_h, db_y]):\n",
    "    param -= lr * dparam\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "| **Advantages**                                                                      | **Limitations**                                                                                                         |\n",
    "| ----------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| - Captures temporal dependencies in sequences.<br>- Allows gradient-based learning. | - Computationally expensive.<br>- Suffers from vanishing/exploding gradients.<br>- Memory-intensive for long sequences. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed8d53",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
