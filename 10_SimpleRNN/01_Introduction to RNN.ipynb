{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5174ef9",
   "metadata": {},
   "source": [
    "Below is a structured breakdown covering **Introduction to RNN** and **Comparison between ANN and RNN**, as per your request:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Introduction to Recurrent Neural Networks (RNNs)**\n",
    "\n",
    "| **Aspect**                      | **Details**                                                                                                                                                                                                                                     |\n",
    "| ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**                  | Recurrent Neural Networks (RNNs) are specialized neural networks designed for **sequential data processing**. Unlike traditional neural networks, they have **loops** in their architecture, allowing information to persist across time steps. |\n",
    "| **Core Concept**                | At each step, an RNN takes the **current input** $x_t$ and the **previous hidden state** $h_{t-1}$ to produce a new hidden state $h_t$ and an output $y_t$. This enables **memory of past inputs**.                                             |\n",
    "| **Mathematical Representation** | $h_t = f(W_h h_{t-1} + W_x x_t + b)$<br>$y_t = g(W_y h_t + c)$                                                                                                                                                                                  |\n",
    "| **Key Features**                | - Maintains **temporal context**.<br>- Works on **variable-length input sequences**.<br>- Uses **shared weights** across time steps.<br>- Can model **time-dependent relationships**.                                                           |\n",
    "| **Example Domains**             | Natural Language Processing, Time Series Forecasting, Speech Recognition, Sequential Event Prediction.                                                                                                                                          |\n",
    "| **Limitations**                 | - Suffers from **vanishing and exploding gradients**.<br>- Struggles with **long-term dependencies**.<br>- Training is computationally intensive for long sequences.                                                                            |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. ANN vs RNN (Artificial Neural Network vs Recurrent Neural Network)**\n",
    "\n",
    "| **Feature**             | **ANN (Artificial Neural Network)**                                  | **RNN (Recurrent Neural Network)**                                                        |\n",
    "| ----------------------- | -------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| **Architecture**        | Feedforward; data moves in one direction (input → hidden → output).  | Recurrent; has feedback loops allowing previous outputs to influence future computations. |\n",
    "| **Data Handling**       | Works on **independent, fixed-size input data** (no inherent order). | Designed for **sequential/temporal data** where order matters.                            |\n",
    "| **Memory**              | No internal memory; each input is processed independently.           | Has a **hidden state** that stores contextual information across time steps.              |\n",
    "| **Weight Sharing**      | Weights are specific to each layer connection.                       | Same weights are shared across all time steps.                                            |\n",
    "| **Applications**        | Image classification, basic regression, tabular data prediction.     | NLP (text generation, translation), time series forecasting, speech recognition.          |\n",
    "| **Training Complexity** | Simpler; backpropagation is straightforward.                         | Uses **Backpropagation Through Time (BPTT)**, making training more complex.               |\n",
    "| **Limitations**         | Cannot model sequential dependencies.                                | Faces vanishing/exploding gradient issues in long sequences.                              |\n",
    "| **Example**             | Predict house price from features.                                   | Predict next word in a sentence based on previous words.                                  |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a52d01",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
