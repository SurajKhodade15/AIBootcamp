{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df32922",
   "metadata": {},
   "source": [
    "## üìò **Theory: Introduction to NLP (Natural Language Processing)**\n",
    "\n",
    "**Definition:**\n",
    "Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) and Computational Linguistics that focuses on enabling machines to **read, understand, and generate human languages**.\n",
    "\n",
    "* It bridges **human communication** and **computer understanding**.\n",
    "* Applications range from **chatbots** to **machine translation** to **sentiment analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why NLP is Important?**\n",
    "\n",
    "* Human language is **ambiguous**, **context-dependent**, and **complex**.\n",
    "* NLP allows computers to extract meaning, insights, and patterns from natural text or speech.\n",
    "* Powers **search engines, virtual assistants, recommendation systems**, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Core Tasks in NLP**\n",
    "\n",
    "| Task                               | Example                                                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- |\n",
    "| **Tokenization**                   | Splitting text into words/sentences.                    |\n",
    "| **Part-of-Speech Tagging (POS)**   | Identifying nouns, verbs, adjectives, etc.              |\n",
    "| **Named Entity Recognition (NER)** | Detecting entities (e.g., names, dates, locations).     |\n",
    "| **Text Classification**            | Spam detection, sentiment analysis.                     |\n",
    "| **Machine Translation**            | Translating between languages (e.g., English ‚Üí French). |\n",
    "| **Text Summarization**             | Creating concise summaries from long documents.         |\n",
    "| **Question Answering**             | Chatbots, virtual assistants (e.g., Siri, Alexa).       |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Components of NLP**\n",
    "\n",
    "| Component              | Description                                                   |\n",
    "| ---------------------- | ------------------------------------------------------------- |\n",
    "| **Linguistics**        | Understanding grammar, syntax, and semantics.                 |\n",
    "| **Text Preprocessing** | Cleaning and preparing data (stopwords, stemming, etc.).      |\n",
    "| **Feature Extraction** | Representing text numerically (BoW, TF-IDF, Word Embeddings). |\n",
    "| **Modeling**           | Applying ML/DL models for predictions.                        |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Approaches in NLP**\n",
    "\n",
    "1. **Rule-based Systems** ‚Äì Use manually created linguistic rules.\n",
    "2. **Statistical NLP** ‚Äì Uses machine learning models on large corpora (e.g., n-gram models).\n",
    "3. **Neural NLP** ‚Äì Uses deep learning architectures (RNN, LSTM, Transformer models like BERT, GPT).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Applications of NLP**\n",
    "\n",
    "* Search engines (Google, Bing)\n",
    "* Virtual assistants (Alexa, Siri, Google Assistant)\n",
    "* Chatbots & Customer Service automation\n",
    "* Machine translation (Google Translate)\n",
    "* Sentiment analysis (social media monitoring)\n",
    "* Text summarization (news apps, legal document processing)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question                                | Answer                                                                           |\n",
    "| --------------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| What is NLP?                            | A branch of AI that deals with interaction between computers and human language. |\n",
    "| Give some examples of NLP applications. | Chatbots, translation, voice assistants, sentiment analysis.                     |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                                    | Answer                                                                                    |\n",
    "| ------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| What are the main steps in an NLP pipeline? | Text preprocessing ‚Üí Feature extraction ‚Üí Model training ‚Üí Evaluation.                    |\n",
    "| What is the biggest challenge in NLP?       | Handling ambiguity, context, sarcasm, and multilingual complexities.                      |\n",
    "| What is the difference between NLP and NLU? | NLP processes language; NLU (Natural Language Understanding) focuses on deriving meaning. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                                  | Answer                                                                                   |\n",
    "| --------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| How does context affect NLP models?                       | Many words have multiple meanings, requiring context-aware models (e.g., transformers).  |\n",
    "| How have transformers revolutionized NLP?                 | They use attention mechanisms to capture long-range dependencies efficiently.            |\n",
    "| What is the difference between NLP and Speech Processing? | NLP deals with text, while speech processing deals with audio signals converted to text. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Basic NLP with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing allows computers to understand human language!\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "print(\"Filtered Words:\", filtered)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This covers the **Introduction to NLP, its importance, tasks, applications, interview questions, and a simple example**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163bee6",
   "metadata": {},
   "source": [
    "## üìò **Theory: Introduction to NLP (Natural Language Processing)**\n",
    "\n",
    "**Definition:**\n",
    "Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) and Computational Linguistics that focuses on enabling machines to **read, understand, and generate human languages**.\n",
    "\n",
    "* It bridges **human communication** and **computer understanding**.\n",
    "* Applications range from **chatbots** to **machine translation** to **sentiment analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why NLP is Important?**\n",
    "\n",
    "* Human language is **ambiguous**, **context-dependent**, and **complex**.\n",
    "* NLP allows computers to extract meaning, insights, and patterns from natural text or speech.\n",
    "* Powers **search engines, virtual assistants, recommendation systems**, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Core Tasks in NLP**\n",
    "\n",
    "| Task                               | Example                                                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- |\n",
    "| **Tokenization**                   | Splitting text into words/sentences.                    |\n",
    "| **Part-of-Speech Tagging (POS)**   | Identifying nouns, verbs, adjectives, etc.              |\n",
    "| **Named Entity Recognition (NER)** | Detecting entities (e.g., names, dates, locations).     |\n",
    "| **Text Classification**            | Spam detection, sentiment analysis.                     |\n",
    "| **Machine Translation**            | Translating between languages (e.g., English ‚Üí French). |\n",
    "| **Text Summarization**             | Creating concise summaries from long documents.         |\n",
    "| **Question Answering**             | Chatbots, virtual assistants (e.g., Siri, Alexa).       |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Components of NLP**\n",
    "\n",
    "| Component              | Description                                                   |\n",
    "| ---------------------- | ------------------------------------------------------------- |\n",
    "| **Linguistics**        | Understanding grammar, syntax, and semantics.                 |\n",
    "| **Text Preprocessing** | Cleaning and preparing data (stopwords, stemming, etc.).      |\n",
    "| **Feature Extraction** | Representing text numerically (BoW, TF-IDF, Word Embeddings). |\n",
    "| **Modeling**           | Applying ML/DL models for predictions.                        |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Approaches in NLP**\n",
    "\n",
    "1. **Rule-based Systems** ‚Äì Use manually created linguistic rules.\n",
    "2. **Statistical NLP** ‚Äì Uses machine learning models on large corpora (e.g., n-gram models).\n",
    "3. **Neural NLP** ‚Äì Uses deep learning architectures (RNN, LSTM, Transformer models like BERT, GPT).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Applications of NLP**\n",
    "\n",
    "* Search engines (Google, Bing)\n",
    "* Virtual assistants (Alexa, Siri, Google Assistant)\n",
    "* Chatbots & Customer Service automation\n",
    "* Machine translation (Google Translate)\n",
    "* Sentiment analysis (social media monitoring)\n",
    "* Text summarization (news apps, legal document processing)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question                                | Answer                                                                           |\n",
    "| --------------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| What is NLP?                            | A branch of AI that deals with interaction between computers and human language. |\n",
    "| Give some examples of NLP applications. | Chatbots, translation, voice assistants, sentiment analysis.                     |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                                    | Answer                                                                                    |\n",
    "| ------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| What are the main steps in an NLP pipeline? | Text preprocessing ‚Üí Feature extraction ‚Üí Model training ‚Üí Evaluation.                    |\n",
    "| What is the biggest challenge in NLP?       | Handling ambiguity, context, sarcasm, and multilingual complexities.                      |\n",
    "| What is the difference between NLP and NLU? | NLP processes language; NLU (Natural Language Understanding) focuses on deriving meaning. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                                  | Answer                                                                                   |\n",
    "| --------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| How does context affect NLP models?                       | Many words have multiple meanings, requiring context-aware models (e.g., transformers).  |\n",
    "| How have transformers revolutionized NLP?                 | They use attention mechanisms to capture long-range dependencies efficiently.            |\n",
    "| What is the difference between NLP and Speech Processing? | NLP deals with text, while speech processing deals with audio signals converted to text. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Basic NLP with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing allows computers to understand human language!\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "print(\"Filtered Words:\", filtered)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This covers the **Introduction to NLP, its importance, tasks, applications, interview questions, and a simple example**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07bcb3f",
   "metadata": {},
   "source": [
    "## üìò **Theory: Stemming in NLP**\n",
    "\n",
    "**Definition:**\n",
    "**Stemming** is a text preprocessing technique that reduces words to their **root** or **base form** (called a **stem**) by stripping suffixes or prefixes.\n",
    "\n",
    "* It does **not** guarantee that the resulting stem is a valid dictionary word.\n",
    "* For example:\n",
    "\n",
    "  * `\"running\"` ‚Üí `\"run\"`\n",
    "  * `\"happiness\"` ‚Üí `\"happi\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why Stemming is Important?**\n",
    "\n",
    "* Reduces vocabulary size by grouping similar words.\n",
    "* Improves efficiency for NLP tasks like search, classification, and indexing.\n",
    "* Helps treat different word forms as the same feature.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Common Stemming Algorithms**\n",
    "\n",
    "| Algorithm             | Description                                                                          |\n",
    "| --------------------- | ------------------------------------------------------------------------------------ |\n",
    "| **Porter Stemmer**    | Most commonly used; applies a set of rules to remove suffixes.                       |\n",
    "| **Snowball Stemmer**  | Improved version of Porter Stemmer; more aggressive and supports multiple languages. |\n",
    "| **Lancaster Stemmer** | Even more aggressive; may over-stem words.                                           |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Limitations of Stemming**\n",
    "\n",
    "* Often **over-stems** (e.g., `\"university\"` ‚Üí `\"univers\"`).\n",
    "* Sometimes **under-stems** (fails to reduce `\"better\"` to `\"good\"`).\n",
    "* Does not consider context or grammar.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question                 | Answer                                                                      |\n",
    "| ------------------------ | --------------------------------------------------------------------------- |\n",
    "| What is stemming?        | A process to reduce words to their base form by removing suffixes/prefixes. |\n",
    "| Why use stemming in NLP? | To normalize words and reduce feature space.                                |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                                       | Answer                                                                                                                            |\n",
    "| ---------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Difference between stemming and lemmatization? | Stemming is rule-based and may produce non-dictionary stems, while lemmatization uses linguistic knowledge to return valid words. |\n",
    "| Which stemmer is commonly used?                | Porter Stemmer (fast and widely implemented).                                                                                     |\n",
    "| When should you avoid stemming?                | When the exact word form is important, like in text generation or sentiment analysis.                                             |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                              | Answer                                                                                |\n",
    "| ----------------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| Why does stemming sometimes harm NLP performance?     | Because over-stemming can merge unrelated words, leading to loss of semantic meaning. |\n",
    "| How does stemming impact vectorization (TF-IDF, BoW)? | It reduces dimensionality by merging similar words, but may introduce ambiguity.      |\n",
    "| Can stemming be language-specific?                    | Yes, different languages require different rule sets.                                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Stemming with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "# Sample text\n",
    "words = [\"running\", \"flies\", \"happiness\", \"studies\", \"better\"]\n",
    "\n",
    "# Apply stemming\n",
    "print(\"Porter Stemmer Results:\")\n",
    "for w in words:\n",
    "    print(w, \"->\", porter.stem(w))\n",
    "\n",
    "print(\"\\nSnowball Stemmer Results:\")\n",
    "for w in words:\n",
    "    print(w, \"->\", snowball.stem(w))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Porter Stemmer Results:\n",
    "running   -> run\n",
    "flies     -> fli\n",
    "happiness -> happi\n",
    "studies   -> studi\n",
    "better    -> better\n",
    "\n",
    "Snowball Stemmer Results:\n",
    "running   -> run\n",
    "flies     -> fli\n",
    "happiness -> happi\n",
    "studies   -> studi\n",
    "better    -> better\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why \"better\" is unchanged?**\n",
    "\n",
    "Because stemming is rule-based and does not use a dictionary to replace words with their root meaning. Hence, `\"better\"` stays the same rather than converting to `\"good\"`, which would require **lemmatization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f990285",
   "metadata": {},
   "source": [
    "## üìò **Theory: Lemmatization in NLP**\n",
    "\n",
    "**Definition:**\n",
    "**Lemmatization** is a text normalization technique that reduces words to their **base or dictionary form** (called a **lemma**) using **linguistic knowledge**.\n",
    "\n",
    "* Unlike stemming, it ensures that the reduced word is a **valid word**.\n",
    "* Example:\n",
    "\n",
    "  * `\"running\"` ‚Üí `\"run\"`\n",
    "  * `\"better\"` ‚Üí `\"good\"`\n",
    "  * `\"studies\"` ‚Üí `\"study\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why Lemmatization is Important?**\n",
    "\n",
    "* Provides **meaningful** base forms by considering grammar (POS tags) and vocabulary.\n",
    "* Improves the quality of NLP features compared to stemming.\n",
    "* Helps models generalize better by grouping words with the same meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **How Lemmatization Works?**\n",
    "\n",
    "* Uses a **lexicon** (dictionary) and **morphological analysis** to correctly map words.\n",
    "* Requires knowing the **Part of Speech (POS)** of the word to determine the correct lemma.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Stemming vs Lemmatization**\n",
    "\n",
    "| Feature      | Stemming                  | Lemmatization                             |\n",
    "| ------------ | ------------------------- | ----------------------------------------- |\n",
    "| **Approach** | Rule-based, cuts suffixes | Dictionary-based with linguistic analysis |\n",
    "| **Output**   | May not be a real word    | Always a valid word                       |\n",
    "| **Example**  | `\"studies\"` ‚Üí `\"studi\"`   | `\"studies\"` ‚Üí `\"study\"`                   |\n",
    "| **Accuracy** | Less accurate             | More accurate                             |\n",
    "| **Speed**    | Faster                    | Slower                                    |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question                           | Answer                                                                         |\n",
    "| ---------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| What is lemmatization?             | A process to reduce words to their dictionary form using linguistic knowledge. |\n",
    "| How is it different from stemming? | Lemmatization returns valid words and considers grammar, unlike stemming.      |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                                            | Answer                                                                                      |\n",
    "| --------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| Why does lemmatization need POS tags?               | Because the lemma depends on the word‚Äôs role (e.g., `\"better\"` as an adjective ‚Üí `\"good\"`). |\n",
    "| When should you prefer lemmatization over stemming? | When semantic meaning is important, such as in search engines or NLP classification tasks.  |\n",
    "| What libraries support lemmatization?               | NLTK, SpaCy, TextBlob, and Stanford NLP.                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                              | Answer                                                                                     |\n",
    "| ----------------------------------------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| Why is lemmatization slower than stemming?            | Because it performs dictionary lookups and morphological analysis.                         |\n",
    "| How does lemmatization improve NLP model performance? | By grouping word variations while preserving semantic meaning, reducing noise in features. |\n",
    "| Can lemmatization handle irregular word forms?        | Yes, because it uses linguistic rules (e.g., `\"went\"` ‚Üí `\"go\"`).                           |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Lemmatization with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"flies\", \"happiness\", \"studies\", \"better\", \"went\"]\n",
    "\n",
    "# Apply lemmatization (default POS is noun)\n",
    "for w in words:\n",
    "    print(w, \"->\", lemmatizer.lemmatize(w))\n",
    "\n",
    "# Applying with POS tags (verb)\n",
    "for w in words:\n",
    "    print(w, \"(verb) ->\", lemmatizer.lemmatize(w, pos='v'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Default Lemmatization (POS = noun):\n",
    "running   -> running\n",
    "flies     -> fly\n",
    "happiness -> happiness\n",
    "studies   -> study\n",
    "better    -> better\n",
    "went      -> went\n",
    "\n",
    "Lemmatization with POS=verb:\n",
    "running   -> run\n",
    "flies     -> fly\n",
    "happiness -> happiness\n",
    "studies   -> study\n",
    "better    -> better\n",
    "went      -> go\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Example: Lemmatization with SpaCy (More Accurate)**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The cats are running and the children went to school happily.\")\n",
    "\n",
    "# Extract lemmas\n",
    "for token in doc:\n",
    "    print(token.text, \"->\", token.lemma_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Sample Output (SpaCy)**\n",
    "\n",
    "```\n",
    "The -> the\n",
    "cats -> cat\n",
    "are -> be\n",
    "running -> run\n",
    "and -> and\n",
    "the -> the\n",
    "children -> child\n",
    "went -> go\n",
    "to -> to\n",
    "school -> school\n",
    "happily -> happily\n",
    ". -> .\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "Lemmatization is a **smarter and linguistically accurate** method than stemming, widely used in modern NLP pipelines where **semantic meaning** is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b5c43",
   "metadata": {},
   "source": [
    "## üìò **Theory: Stopwords in NLP**\n",
    "\n",
    "**Definition:**\n",
    "**Stopwords** are commonly used words in a language (e.g., \"the\", \"is\", \"and\", \"in\") that **do not contribute significant meaning** to the text analysis.\n",
    "\n",
    "* Removing them helps reduce noise and dimensionality in NLP tasks.\n",
    "* Example:\n",
    "\n",
    "  * Original: `\"The cat is running in the garden\"`\n",
    "  * After removing stopwords: `\"cat running garden\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why Remove Stopwords?**\n",
    "\n",
    "* They appear frequently and add little semantic value in many tasks.\n",
    "* Improves computational efficiency by reducing the number of tokens.\n",
    "* Helps models focus on more meaningful words.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **When NOT to Remove Stopwords?**\n",
    "\n",
    "* When stopwords carry important meaning (e.g., in **sentiment analysis**, the word `\"not\"` is critical).\n",
    "* For tasks like **text generation** where all words may be relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Stopword Lists**\n",
    "\n",
    "* NLP libraries like **NLTK**, **SpaCy**, **Scikit-learn** provide predefined stopword lists for multiple languages.\n",
    "* Custom stopword lists can also be created depending on the task.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question            | Answer                                                                         |\n",
    "| ------------------- | ------------------------------------------------------------------------------ |\n",
    "| What are stopwords? | Commonly used words in a language that are often removed in NLP preprocessing. |\n",
    "| Give examples.      | \"the\", \"is\", \"and\", \"on\", \"in\", \"to\".                                          |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                      | Answer                                                                                  |\n",
    "| ----------------------------- | --------------------------------------------------------------------------------------- |\n",
    "| Why do we remove stopwords?   | To reduce noise and dimensionality, improving efficiency.                               |\n",
    "| Are stopwords always removed? | No, in tasks where context matters (e.g., `\"not happy\"`), stopwords should be retained. |\n",
    "| Can stopwords vary by domain? | Yes, custom stopword lists are often created for domain-specific tasks.                 |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                                         | Answer                                                                                                         |\n",
    "| ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |\n",
    "| How do stopwords affect TF-IDF scores?                           | Since stopwords appear frequently across documents, their TF-IDF score is low, minimizing their impact.        |\n",
    "| How do you handle stopwords in languages like Chinese or Arabic? | Use language-specific tokenizers and stopword lists.                                                           |\n",
    "| Can removing stopwords ever harm performance?                    | Yes, if important words like `\"not\"`, `\"never\"`, `\"without\"` are removed in sentiment or negation-based tasks. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Removing Stopwords with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"The cat is running in the beautiful garden and it is playing happily.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Original Tokens: ['the', 'cat', 'is', 'running', 'in', 'the', 'beautiful', 'garden', 'and', 'it', 'is', 'playing', 'happily', '.']\n",
    "Filtered Tokens: ['cat', 'running', 'beautiful', 'garden', 'playing', 'happily']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Example: Stopword Removal with SpaCy**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The cat is running in the garden happily and it enjoys playing.\")\n",
    "\n",
    "filtered = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "print(\"Tokens after Stopword Removal:\", filtered)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Sample Output**\n",
    "\n",
    "```\n",
    "Tokens after Stopword Removal: ['cat', 'running', 'garden', 'happily', 'enjoys', 'playing']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "Stopwords are usually removed in preprocessing to **reduce noise** and **speed up processing**, but in tasks where they contribute to meaning, they should be retained or selectively removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c8e2b",
   "metadata": {},
   "source": [
    "## üìò **Theory: POS (Part-of-Speech) Tagging in NLP**\n",
    "\n",
    "**Definition:**\n",
    "**Part-of-Speech (POS) Tagging** is the process of **assigning grammatical categories** (such as noun, verb, adjective) to each word in a sentence.\n",
    "\n",
    "* Example:\n",
    "\n",
    "  * `\"The cat sleeps\"`\n",
    "  * POS Tags ‚Üí `The (DET)`, `cat (NOUN)`, `sleeps (VERB)`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why POS Tagging is Important?**\n",
    "\n",
    "* Helps understand the **syntactic** structure of a sentence.\n",
    "* Essential for downstream NLP tasks such as Named Entity Recognition (NER), parsing, and information extraction.\n",
    "* Improves semantic analysis by clarifying word roles (e.g., `\"book\"` as a noun vs. `\"book\"` as a verb).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Common POS Tags**\n",
    "\n",
    "| Tag     | Meaning           | Example          |\n",
    "| ------- | ----------------- | ---------------- |\n",
    "| **NN**  | Noun (singular)   | dog, cat         |\n",
    "| **NNS** | Noun (plural)     | dogs, cars       |\n",
    "| **VB**  | Verb (base form)  | run, eat         |\n",
    "| **VBD** | Verb (past tense) | ran, ate         |\n",
    "| **JJ**  | Adjective         | beautiful, fast  |\n",
    "| **RB**  | Adverb            | quickly, happily |\n",
    "| **PRP** | Pronoun           | he, she, it      |\n",
    "| **DT**  | Determiner        | the, a, an       |\n",
    "| **IN**  | Preposition       | in, on, at       |\n",
    "| **CC**  | Conjunction       | and, or, but     |\n",
    "\n",
    "(Full tag set is available in the Penn Treebank tagset.)\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Techniques for POS Tagging**\n",
    "\n",
    "| Method                        | Description                                                                |\n",
    "| ----------------------------- | -------------------------------------------------------------------------- |\n",
    "| **Rule-Based**                | Uses handcrafted grammatical rules.                                        |\n",
    "| **Statistical**               | Uses probabilistic models (e.g., Hidden Markov Models).                    |\n",
    "| **Neural Models**             | Uses deep learning (e.g., BiLSTM, Transformers) for context-aware tagging. |\n",
    "| **Pre-trained NLP Libraries** | NLTK, SpaCy, Stanford NLP, etc., provide trained POS taggers.              |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question               | Answer                                             |\n",
    "| ---------------------- | -------------------------------------------------- |\n",
    "| What is POS tagging?   | Assigning grammatical tags to words in a sentence. |\n",
    "| Why is it used in NLP? | To understand sentence structure and meaning.      |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                             | Answer                                                   |\n",
    "| ------------------------------------ | -------------------------------------------------------- |\n",
    "| Which POS tagging approaches exist?  | Rule-based, statistical, and machine learning-based.     |\n",
    "| What is an example of POS ambiguity? | `\"can\"` (noun: a container) vs. `\"can\"` (verb: able to). |\n",
    "| Which libraries provide POS tagging? | NLTK, SpaCy, Stanford NLP, CoreNLP.                      |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                              | Answer                                                                              |\n",
    "| ----------------------------------------------------- | ----------------------------------------------------------------------------------- |\n",
    "| How do modern models (like BERT) improve POS tagging? | They use contextual embeddings, allowing better disambiguation of words.            |\n",
    "| How does POS tagging help in NER?                     | It helps identify the grammatical role of entities, improving recognition accuracy. |\n",
    "| How is POS tagging evaluated?                         | Using accuracy, precision, recall, and F1-score on labeled corpora.                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: POS Tagging with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), \n",
    "           ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), \n",
    "           ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Example: POS Tagging with SpaCy (More Accurate)**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "# Display tokens and their POS tags\n",
    "for token in doc:\n",
    "    print(token.text, \"->\", token.pos_, \"/\", token.tag_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Sample Output**\n",
    "\n",
    "```\n",
    "The -> DET / DT\n",
    "quick -> ADJ / JJ\n",
    "brown -> ADJ / JJ\n",
    "fox -> NOUN / NN\n",
    "jumps -> VERB / VBZ\n",
    "over -> ADP / IN\n",
    "the -> DET / DT\n",
    "lazy -> ADJ / JJ\n",
    "dog -> NOUN / NN\n",
    ". -> PUNCT / .\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "POS Tagging is a fundamental step in NLP pipelines, enabling **syntactic understanding** and improving tasks such as **parsing, information extraction, and semantic analysis**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e6a5d",
   "metadata": {},
   "source": [
    "## üìò **Theory: Named Entity Recognition (NER)**\n",
    "\n",
    "**Definition:**\n",
    "**Named Entity Recognition (NER)** is the NLP task of **identifying and classifying key entities** in text into predefined categories such as **people, organizations, locations, dates, etc.**\n",
    "\n",
    "* For example, in the sentence:\n",
    "  `\"Apple was founded by Steve Jobs in California.\"`\n",
    "\n",
    "  * NER tags:\n",
    "\n",
    "    * `Apple` ‚Üí Organization\n",
    "    * `Steve Jobs` ‚Üí Person\n",
    "    * `California` ‚Üí Location\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why NER is Important?**\n",
    "\n",
    "* Extracts structured information from unstructured text.\n",
    "* Used in search engines, question answering, customer support, and information retrieval.\n",
    "* Helps in building knowledge graphs and summarizing documents.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Common Entity Types**\n",
    "\n",
    "| Entity Type      | Examples                             |\n",
    "| ---------------- | ------------------------------------ |\n",
    "| **PERSON**       | Steve Jobs, Barack Obama             |\n",
    "| **ORGANIZATION** | Apple, Google, United Nations        |\n",
    "| **LOCATION**     | New York, Paris, India               |\n",
    "| **DATE**         | January 1, 2020, Monday              |\n",
    "| **TIME**         | 10 AM, midnight                      |\n",
    "| **MONEY**        | \\$100, 50 euros                      |\n",
    "| **PERCENT**      | 10%, 75 percent                      |\n",
    "| **MISC**         | Other entities like products, events |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Techniques for NER**\n",
    "\n",
    "| Approach               | Description                                        |\n",
    "| ---------------------- | -------------------------------------------------- |\n",
    "| **Rule-Based**         | Uses handcrafted rules and patterns.               |\n",
    "| **Statistical Models** | CRF, HMM trained on annotated corpora.             |\n",
    "| **Neural Networks**    | LSTM, BiLSTM with Conditional Random Fields (CRF). |\n",
    "| **Transformer Models** | BERT, RoBERTa fine-tuned for NER.                  |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question                          | Answer                                                                   |\n",
    "| --------------------------------- | ------------------------------------------------------------------------ |\n",
    "| What is Named Entity Recognition? | Extracting and classifying entities like names, places, dates from text. |\n",
    "| Why is NER useful?                | Converts unstructured text into structured data for analysis.            |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                           | Answer                                                           |\n",
    "| ---------------------------------- | ---------------------------------------------------------------- |\n",
    "| What are common challenges in NER? | Ambiguity, overlapping entities, domain adaptation.              |\n",
    "| How does context affect NER?       | Same word can represent different entities depending on context. |\n",
    "| What datasets are used for NER?    | CoNLL-2003, OntoNotes, ACE datasets.                             |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                             | Answer                                                                      |\n",
    "| ---------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| How do transformer models improve NER?               | Capture long-range dependencies and contextual meaning better.              |\n",
    "| What is the role of CRF in NER models?               | Ensures valid tag sequences by modeling label dependencies.                 |\n",
    "| How to handle nested or overlapping entities in NER? | Advanced models and tagging schemes like layered CRFs or span-based models. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: NER with SpaCy**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load pre-trained English model with NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple was founded by Steve Jobs in California in 1976.\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities with labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Apple -> ORG\n",
    "Steve Jobs -> PERSON\n",
    "California -> GPE\n",
    "1976 -> DATE\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "NER is a critical component of many NLP systems to extract **meaningful entities** for structured knowledge and further automated processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cade57",
   "metadata": {},
   "source": [
    "## üìò **Theory: Named Entity Recognition (NER)**\n",
    "\n",
    "**Definition:**\n",
    "**Named Entity Recognition (NER)** is the NLP task of **identifying and classifying key entities** in text into predefined categories such as **people, organizations, locations, dates, etc.**\n",
    "\n",
    "* For example, in the sentence:\n",
    "  `\"Apple was founded by Steve Jobs in California.\"`\n",
    "\n",
    "  * NER tags:\n",
    "\n",
    "    * `Apple` ‚Üí Organization\n",
    "    * `Steve Jobs` ‚Üí Person\n",
    "    * `California` ‚Üí Location\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why NER is Important?**\n",
    "\n",
    "* Extracts structured information from unstructured text.\n",
    "* Used in search engines, question answering, customer support, and information retrieval.\n",
    "* Helps in building knowledge graphs and summarizing documents.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Common Entity Types**\n",
    "\n",
    "| Entity Type      | Examples                             |\n",
    "| ---------------- | ------------------------------------ |\n",
    "| **PERSON**       | Steve Jobs, Barack Obama             |\n",
    "| **ORGANIZATION** | Apple, Google, United Nations        |\n",
    "| **LOCATION**     | New York, Paris, India               |\n",
    "| **DATE**         | January 1, 2020, Monday              |\n",
    "| **TIME**         | 10 AM, midnight                      |\n",
    "| **MONEY**        | \\$100, 50 euros                      |\n",
    "| **PERCENT**      | 10%, 75 percent                      |\n",
    "| **MISC**         | Other entities like products, events |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Techniques for NER**\n",
    "\n",
    "| Approach               | Description                                        |\n",
    "| ---------------------- | -------------------------------------------------- |\n",
    "| **Rule-Based**         | Uses handcrafted rules and patterns.               |\n",
    "| **Statistical Models** | CRF, HMM trained on annotated corpora.             |\n",
    "| **Neural Networks**    | LSTM, BiLSTM with Conditional Random Fields (CRF). |\n",
    "| **Transformer Models** | BERT, RoBERTa fine-tuned for NER.                  |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question                          | Answer                                                                   |\n",
    "| --------------------------------- | ------------------------------------------------------------------------ |\n",
    "| What is Named Entity Recognition? | Extracting and classifying entities like names, places, dates from text. |\n",
    "| Why is NER useful?                | Converts unstructured text into structured data for analysis.            |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                           | Answer                                                           |\n",
    "| ---------------------------------- | ---------------------------------------------------------------- |\n",
    "| What are common challenges in NER? | Ambiguity, overlapping entities, domain adaptation.              |\n",
    "| How does context affect NER?       | Same word can represent different entities depending on context. |\n",
    "| What datasets are used for NER?    | CoNLL-2003, OntoNotes, ACE datasets.                             |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                             | Answer                                                                      |\n",
    "| ---------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| How do transformer models improve NER?               | Capture long-range dependencies and contextual meaning better.              |\n",
    "| What is the role of CRF in NER models?               | Ensures valid tag sequences by modeling label dependencies.                 |\n",
    "| How to handle nested or overlapping entities in NER? | Advanced models and tagging schemes like layered CRFs or span-based models. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: NER with SpaCy**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load pre-trained English model with NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple was founded by Steve Jobs in California in 1976.\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities with labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Apple -> ORG\n",
    "Steve Jobs -> PERSON\n",
    "California -> GPE\n",
    "1976 -> DATE\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "NER is a critical component of many NLP systems to extract **meaningful entities** for structured knowledge and further automated processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da5659",
   "metadata": {},
   "source": [
    "## üìò **Theory: Bag of Words (BoW)**\n",
    "\n",
    "**Definition:**\n",
    "**Bag of Words (BoW)** is a text representation technique in NLP that converts a text document into a **vector of word counts or frequencies**, ignoring grammar and word order but keeping multiplicity.\n",
    "\n",
    "* The document is represented as a ‚Äúbag‚Äù (multiset) of its words.\n",
    "* Example:\n",
    "\n",
    "  * Documents:\n",
    "\n",
    "    * Doc1: `\"I love machine learning\"`\n",
    "    * Doc2: `\"Machine learning is fun\"`\n",
    "  * Vocabulary: `[I, love, machine, learning, is, fun]`\n",
    "  * Doc1 BoW vector: `[1, 1, 1, 1, 0, 0]`\n",
    "  * Doc2 BoW vector: `[0, 0, 1, 1, 1, 1]`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why BoW is Important?**\n",
    "\n",
    "* Converts text into fixed-length numeric vectors usable by ML algorithms.\n",
    "* Simple and effective baseline for text classification and information retrieval.\n",
    "* Basis for more advanced techniques like TF-IDF and word embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Limitations**\n",
    "\n",
    "* Ignores word order and context.\n",
    "* Vocabulary size can be very large, causing sparsity.\n",
    "* Cannot capture semantics or polysemy.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question                      | Answer                                                            |\n",
    "| ----------------------------- | ----------------------------------------------------------------- |\n",
    "| What is Bag of Words?         | A vector representation counting word occurrences ignoring order. |\n",
    "| Does BoW consider word order? | No, it treats text as an unordered collection of words.           |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                                   | Answer                                                                         |\n",
    "| ------------------------------------------ | ------------------------------------------------------------------------------ |\n",
    "| How do you handle large vocabulary in BoW? | Use feature selection, stopword removal, or limit vocabulary size.             |\n",
    "| How is BoW different from TF-IDF?          | BoW counts raw frequency; TF-IDF weights words by importance across documents. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                               | Answer                                                                |\n",
    "| ------------------------------------------------------ | --------------------------------------------------------------------- |\n",
    "| How to handle sparsity in BoW vectors?                 | Dimensionality reduction (e.g., PCA), or switch to dense embeddings.  |\n",
    "| Can BoW be used for languages with complex morphology? | It can, but may require preprocessing like stemming or lemmatization. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Bag of Words with scikit-learn**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array and print feature names\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", bow_matrix.toarray())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Vocabulary: ['fun' 'is' 'learning' 'love' 'machine']\n",
    "BoW Matrix:\n",
    " [[0 0 1 1 1]\n",
    "  [1 1 1 0 1]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "Bag of Words is a foundational NLP technique that enables conversion of text into a structured numerical format for machine learning, providing a simple yet effective baseline for many applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12c135",
   "metadata": {},
   "source": [
    "## üìò **Theory: N-Grams in NLP**\n",
    "\n",
    "**Definition:**\n",
    "**N-Grams** are contiguous sequences of **n** items (usually words or characters) extracted from a text.\n",
    "\n",
    "* For example, given the sentence:\n",
    "  `\"Machine learning is fun\"`\n",
    "* The n-grams are:\n",
    "\n",
    "  * **Unigrams (1-gram):** `\"Machine\"`, `\"learning\"`, `\"is\"`, `\"fun\"`\n",
    "  * **Bigrams (2-grams):** `\"Machine learning\"`, `\"learning is\"`, `\"is fun\"`\n",
    "  * **Trigrams (3-grams):** `\"Machine learning is\"`, `\"learning is fun\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why N-Grams are Useful?**\n",
    "\n",
    "* Capture some **context** by considering word sequences, unlike Bag of Words.\n",
    "* Improve models by encoding common phrases or patterns.\n",
    "* Used in language modeling, text classification, and speech recognition.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Types of N-Grams**\n",
    "\n",
    "| N | Name    | Example                 |\n",
    "| - | ------- | ----------------------- |\n",
    "| 1 | Unigram | Single words            |\n",
    "| 2 | Bigram  | Two consecutive words   |\n",
    "| 3 | Trigram | Three consecutive words |\n",
    "| n | N-gram  | n consecutive words     |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Limitations**\n",
    "\n",
    "* Higher n leads to **data sparsity** and increased dimensionality.\n",
    "* Longer n-grams capture more context but require more data.\n",
    "* Does not capture long-range dependencies beyond n.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "### ‚úÖ **Basic Level**\n",
    "\n",
    "| Question           | Answer                                      |\n",
    "| ------------------ | ------------------------------------------- |\n",
    "| What is an n-gram? | A contiguous sequence of n words or tokens. |\n",
    "| What is a bigram?  | A sequence of two consecutive words.        |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Intermediate Level**\n",
    "\n",
    "| Question                            | Answer                                           |\n",
    "| ----------------------------------- | ------------------------------------------------ |\n",
    "| How do n-grams help in NLP?         | By capturing word order and local context.       |\n",
    "| What challenges arise with large n? | Increased sparsity and computational complexity. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advanced Level**\n",
    "\n",
    "| Question                                     | Answer                                                    |\n",
    "| -------------------------------------------- | --------------------------------------------------------- |\n",
    "| How do you handle sparsity in n-gram models? | Use smoothing techniques like Laplace smoothing.          |\n",
    "| How are n-grams used in language models?     | To predict the next word based on the previous n-1 words. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Generating N-Grams with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentence = \"Machine learning is fun and powerful\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "# Generate bigrams (2-grams)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)\n",
    "\n",
    "# Generate trigrams (3-grams)\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(\"Trigrams:\", trigrams)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Bigrams: [('machine', 'learning'), ('learning', 'is'), ('is', 'fun'), ('fun', 'and'), ('and', 'powerful')]\n",
    "Trigrams: [('machine', 'learning', 'is'), ('learning', 'is', 'fun'), ('is', 'fun', 'and'), ('fun', 'and', 'powerful')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "N-Grams help incorporate **local context** in text representations and are a building block for many NLP applications including language modeling and feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b766d6",
   "metadata": {},
   "source": [
    "## üìò **Theory: N-gram Bag of Words (N-gram BoW)**\n",
    "\n",
    "**Definition:**\n",
    "**N-gram Bag of Words** extends the traditional Bag of Words model by including **contiguous sequences of N words (N-grams)** as features instead of just single words (unigrams).\n",
    "\n",
    "* Instead of representing text as just individual words, it captures **phrases** or **word combinations** to add context.\n",
    "* For example, for the sentence:\n",
    "  `\"I love machine learning\"`\n",
    "\n",
    "  * **Unigram BoW:** counts of `\"I\"`, `\"love\"`, `\"machine\"`, `\"learning\"`\n",
    "  * **Bigram BoW:** counts of `\"I love\"`, `\"love machine\"`, `\"machine learning\"`\n",
    "  * **Combined N-gram BoW (1 to 2):** counts of all unigrams + bigrams\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why Use N-gram BoW?**\n",
    "\n",
    "* Captures **local word order** and some context missing in simple BoW.\n",
    "* Helps distinguish phrases (e.g., `\"New York\"` vs. `\"new\"`, `\"york\"` separately).\n",
    "* Improves performance in text classification, sentiment analysis, and language modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Trade-offs**\n",
    "\n",
    "* Larger vocabulary and **higher dimensionality** compared to unigram BoW.\n",
    "* Increased sparsity can require dimensionality reduction or feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "| Question                              | Answer                                                                              |\n",
    "| ------------------------------------- | ----------------------------------------------------------------------------------- |\n",
    "| What is N-gram Bag of Words?          | A text representation that includes counts of word sequences (N-grams) as features. |\n",
    "| How is it different from unigram BoW? | Unigram BoW counts single words; N-gram BoW also counts sequences of N words.       |\n",
    "| What are challenges with N-gram BoW?  | Curse of dimensionality and sparsity from large feature space.                      |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: N-gram BoW with scikit-learn**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer with ngram_range to include unigrams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"N-gram BoW Matrix:\\n\", X.toarray())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Vocabulary: ['fun' 'is' 'learning' 'learning is' 'love' 'machine' 'machine learning' 'i love']\n",
    "N-gram BoW Matrix:\n",
    " [[0 0 1 0 1 0 0 1]\n",
    "  [1 1 1 1 0 1 1 0]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "N-gram Bag of Words enriches the traditional BoW model by including word sequences, allowing models to better capture context and phrase-level information while balancing complexity and feature size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a9fce",
   "metadata": {},
   "source": [
    "## üìò **Theory: TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "\n",
    "**Definition:**\n",
    "**TF-IDF** is a statistical measure used to evaluate how important a word is to a document in a collection (corpus). It balances the **frequency of a term in a document** with how **common or rare it is across all documents**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Components**\n",
    "\n",
    "| Term                                 | Description                                                                     | Formula                                                                                                                   |\n",
    "| ------------------------------------ | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Term Frequency (TF)**              | How often a term appears in a document.                                         | $\\text{TF}(t,d) = \\frac{\\text{Number of times } t \\text{ appears in } d}{\\text{Total number of terms in } d}$             |\n",
    "| **Inverse Document Frequency (IDF)** | Measures how rare a term is across all documents.                               | $\\text{IDF}(t) = \\log\\frac{N}{1 + \\text{DF}(t)}$ where $N$ is total docs, $\\text{DF}(t)$ is number of docs containing $t$ |\n",
    "| **TF-IDF Score**                     | Product of TF and IDF, highlighting terms frequent in a doc but rare in corpus. | $\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)$                                                                |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why TF-IDF is Important?**\n",
    "\n",
    "* Reduces the impact of common words like ‚Äúthe‚Äù or ‚Äúis‚Äù which appear in many documents.\n",
    "* Highlights **important keywords** in each document.\n",
    "* Widely used for text classification, information retrieval, and search engines.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Limitations**\n",
    "\n",
    "* Assumes word independence (ignores semantics and context).\n",
    "* Can produce sparse and high-dimensional vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "| Question                            | Answer                                                                                      |\n",
    "| ----------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| What does TF-IDF stand for?         | Term Frequency-Inverse Document Frequency.                                                  |\n",
    "| Why not just use term frequency?    | Common words appear frequently but are less informative; IDF down-weights them.             |\n",
    "| How does IDF penalize common words? | Common words appear in many documents, so IDF value decreases, reducing their TF-IDF score. |\n",
    "| What are applications of TF-IDF?    | Text classification, keyword extraction, document similarity, search ranking.               |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: TF-IDF with scikit-learn**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Vocabulary: ['coding' 'fun' 'in' 'is' 'learning' 'love' 'machine' 'python']\n",
    "TF-IDF Matrix:\n",
    " [[0.         0.         0.         0.         0.57973867 0.81480247 0.        0.        ]\n",
    "  [0.         0.70710678 0.         0.70710678 0.5        0.         0.        0.        ]\n",
    "  [0.61761437 0.         0.61761437 0.         0.         0.51184888 0.         0.51184888]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "TF-IDF is a powerful feature extraction technique that balances term frequency with word importance across documents, helping models focus on discriminative words rather than common terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa47597",
   "metadata": {},
   "source": [
    "## üìò **Theory: Word Embedding**\n",
    "\n",
    "**Definition:**\n",
    "**Word Embedding** refers to a set of techniques in NLP that map words or phrases to **dense, continuous vector representations** in a low-dimensional space.\n",
    "\n",
    "* Unlike sparse one-hot vectors, embeddings capture **semantic relationships** between words based on their context.\n",
    "* Words with similar meanings have vectors close to each other in this space.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why Word Embeddings?**\n",
    "\n",
    "* Capture **semantic similarity** (e.g., \"king\" and \"queen\" are related).\n",
    "* Enable machine learning models to understand **context** and **meaning** better.\n",
    "* Solve problems of high dimensionality and sparsity in traditional text representations.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Popular Word Embedding Methods**\n",
    "\n",
    "| Method                                 | Description                                                               |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| **Word2Vec**                           | Predicts words given context (CBOW) or context given word (Skip-gram).    |\n",
    "| **GloVe**                              | Generates embeddings by factorizing word co-occurrence matrices.          |\n",
    "| **FastText**                           | Extends Word2Vec by incorporating subword (character n-gram) information. |\n",
    "| **Contextual embeddings (e.g., BERT)** | Provide dynamic embeddings depending on word context in sentence.         |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Properties**\n",
    "\n",
    "| Property               | Explanation                                                                                          |\n",
    "| ---------------------- | ---------------------------------------------------------------------------------------------------- |\n",
    "| Dense Vectors          | Typically 50-300 dimensions, dense real numbers.                                                     |\n",
    "| Semantic Relationships | Vector arithmetic reflects meaning (e.g., `vec(\"king\") - vec(\"man\") + vec(\"woman\") ‚âà vec(\"queen\")`). |\n",
    "| Pre-trained Models     | Commonly used pretrained embeddings for transfer learning.                                           |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "| Question                                                       | Answer                                                                                                                 |\n",
    "| -------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| What is a word embedding?                                      | A dense vector representation of words capturing semantic similarity.                                                  |\n",
    "| How is Word2Vec different from one-hot encoding?               | Word2Vec produces dense, low-dimensional, and semantically meaningful vectors; one-hot is sparse and high-dimensional. |\n",
    "| What is the difference between CBOW and Skip-gram in Word2Vec? | CBOW predicts a word from its context; Skip-gram predicts context words from a target word.                            |\n",
    "| Why use pre-trained embeddings?                                | To leverage learned semantic patterns from large corpora, improving performance and saving training time.              |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Using Pre-trained Word2Vec with Gensim**\n",
    "\n",
    "```python\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained Word2Vec model (Google News, 300d)\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Vector for a word\n",
    "vector_king = model['king']\n",
    "\n",
    "# Find most similar words to 'king'\n",
    "similar_words = model.most_similar('king', topn=5)\n",
    "\n",
    "print(\"Vector for 'king':\", vector_king[:5])  # Print first 5 dimensions\n",
    "print(\"Top 5 similar words to 'king':\", similar_words)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Sample Output**\n",
    "\n",
    "```\n",
    "Vector for 'king': [ 0.1234, -0.2345, 0.5678, -0.3456, 0.4567]\n",
    "Top 5 similar words to 'king': [('queen', 0.725), ('prince', 0.680), ('monarch', 0.670), ('crown', 0.660), ('kingdom', 0.650)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "Word embeddings revolutionize NLP by transforming words into meaningful numeric vectors that capture semantics, context, and relationships, enabling more intelligent language understanding by ML models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64332d67",
   "metadata": {},
   "source": [
    "## üìò **Theory: Word2Vec**\n",
    "\n",
    "**Definition:**\n",
    "**Word2Vec** is a popular neural network-based technique to create **word embeddings**‚Äîdense vector representations of words that capture semantic and syntactic relationships.\n",
    "\n",
    "* Introduced by Mikolov et al. at Google in 2013.\n",
    "* It learns word vectors by predicting words in context using a large text corpus.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Two Main Architectures**\n",
    "\n",
    "| Architecture                       | Description                                                              | Objective                                          |\n",
    "| ---------------------------------- | ------------------------------------------------------------------------ | -------------------------------------------------- |\n",
    "| **CBOW (Continuous Bag of Words)** | Predicts the **target word** based on its **surrounding context words**. | Given context words, predict the center word.      |\n",
    "| **Skip-Gram**                      | Predicts **context words** given the **target word**.                    | Given a word, predict its neighbors in the window. |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **How Word2Vec Works**\n",
    "\n",
    "* The model trains shallow neural networks to **maximize the probability** of context words given a target word (Skip-Gram) or vice versa (CBOW).\n",
    "* The learned weights in the hidden layer become the word vectors (embeddings).\n",
    "* Word vectors capture semantic similarity (e.g., ‚Äúking‚Äù and ‚Äúqueen‚Äù vectors are close).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Features**\n",
    "\n",
    "| Feature                     | Explanation                                                                        |\n",
    "| --------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| Efficient training          | Uses techniques like Negative Sampling and Hierarchical Softmax for fast training. |\n",
    "| Captures semantic relations | Supports vector arithmetic (e.g., `king - man + woman ‚âà queen`).                   |\n",
    "| Dimensionality              | Typically 100‚Äì300 dimensions for embeddings.                                       |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "| Question                                          | Answer                                                                                                |\n",
    "| ------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| What is Word2Vec?                                 | A technique for learning dense word embeddings using neural networks.                                 |\n",
    "| What‚Äôs the difference between CBOW and Skip-Gram? | CBOW predicts a word from context; Skip-Gram predicts context from a word.                            |\n",
    "| How does Word2Vec capture semantic relationships? | Through training on word co-occurrence, embeddings encode similarity and relations.                   |\n",
    "| What is Negative Sampling?                        | A technique to efficiently train Word2Vec by updating only a subset of weights for negative examples. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Python Example: Using Pre-trained Word2Vec with Gensim**\n",
    "\n",
    "```python\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained Word2Vec model (Google News vectors)\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Get vector for 'king'\n",
    "vector_king = model['king']\n",
    "\n",
    "# Find top 5 most similar words to 'king'\n",
    "similar_words = model.most_similar('king', topn=5)\n",
    "\n",
    "print(\"Vector for 'king' (first 5 dimensions):\", vector_king[:5])\n",
    "print(\"Top 5 similar words to 'king':\", similar_words)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Vector for 'king' (first 5 dimensions): [0.123, -0.234, 0.567, -0.345, 0.456]\n",
    "Top 5 similar words to 'king': [('queen', 0.725), ('prince', 0.680), ('monarch', 0.670), ('crown', 0.660), ('kingdom', 0.650)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "Word2Vec is a foundational method for learning meaningful word embeddings that encode semantic relationships, providing the basis for many modern NLP applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d194106",
   "metadata": {},
   "source": [
    "## üìò **Theory: Average Word2Vec (Avg Word2Vec)**\n",
    "\n",
    "**Definition:**\n",
    "**Average Word2Vec** is a simple technique to create a fixed-length vector representation for an entire document or sentence by **averaging the Word2Vec embeddings** of all the words it contains.\n",
    "\n",
    "* Since Word2Vec produces vectors for individual words, averaging combines these into a single vector representing the whole text.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why Use Average Word2Vec?**\n",
    "\n",
    "* Provides a **simple and effective sentence/document embedding** from pre-trained word vectors.\n",
    "* Captures the overall semantic meaning of the text by aggregating word meanings.\n",
    "* Computationally efficient baseline for text classification, clustering, or similarity tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Limitations**\n",
    "\n",
    "* Ignores word order and syntax.\n",
    "* All words contribute equally‚Äîimportant words are not weighted differently.\n",
    "* Cannot capture complex compositional meanings or context beyond word level.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Interview Insights**\n",
    "\n",
    "| Question                                | Answer                                                                                                                 |\n",
    "| --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| What is Average Word2Vec?               | Averaging word vectors to represent sentences/documents as single vectors.                                             |\n",
    "| Why not just use word vectors directly? | Word vectors are for individual words; averaging provides fixed-length input for models requiring it.                  |\n",
    "| What are alternatives to Avg Word2Vec?  | Weighted averaging (e.g., using TF-IDF weights), advanced sentence embeddings like BERT or Universal Sentence Encoder. |\n",
    "\n",
    "---\n",
    "\n",
    "## üêç **Simple Python Example: Average Word2Vec**\n",
    "\n",
    "```python\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"I love machine learning\"\n",
    "\n",
    "# Tokenize sentence\n",
    "words = sentence.lower().split()\n",
    "\n",
    "# Get vectors for words in the model's vocabulary\n",
    "word_vectors = [model[word] for word in words if word in model]\n",
    "\n",
    "# Compute average vector\n",
    "if word_vectors:\n",
    "    avg_vector = np.mean(word_vectors, axis=0)\n",
    "    print(\"Average Word2Vec vector shape:\", avg_vector.shape)\n",
    "else:\n",
    "    print(\"No words in vocabulary.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Expected Output**\n",
    "\n",
    "```\n",
    "Average Word2Vec vector shape: (300,)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "Average Word2Vec is a straightforward method to convert variable-length text into fixed-size numeric vectors by aggregating individual word embeddings, providing a baseline for many NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc862b",
   "metadata": {},
   "source": [
    "Certainly, Suraj. Below is a structured, end-to-end pipeline for training a basic NLP model‚Äîspecifically, a text classification model‚Äîusing Python. This framework covers data preprocessing, feature extraction, model training, evaluation, and prediction, following industry-standard practices.\n",
    "\n",
    "---\n",
    "\n",
    "# End-to-End NLP Model Training Pipeline: Text Classification Example\n",
    "\n",
    "### 1. **Data Collection**\n",
    "\n",
    "Collect labeled text data. For example, sentiment analysis dataset with texts labeled positive/negative.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Data Preprocessing**\n",
    "\n",
    "* Text cleaning (lowercasing, removing punctuation)\n",
    "* Tokenization\n",
    "* Stopword removal\n",
    "* Lemmatization or stemming\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample data\n",
    "data = {'text': ['I love this product!', 'This is the worst experience ever.'],\n",
    "        'label': [1, 0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(preprocess)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Feature Extraction: TF-IDF Vectorization**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "y = df['label']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Train-Test Split**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Model Training: Logistic Regression**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Model Evaluation**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Prediction on New Text**\n",
    "\n",
    "```python\n",
    "def predict(text):\n",
    "    processed = preprocess(text)\n",
    "    vector = vectorizer.transform([processed])\n",
    "    prediction = model.predict(vector)\n",
    "    return \"Positive\" if prediction[0] == 1 else \"Negative\"\n",
    "\n",
    "print(predict(\"I really enjoyed the new movie\"))\n",
    "print(predict(\"The service was terrible\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "| Step               | Description                                  |\n",
    "| ------------------ | -------------------------------------------- |\n",
    "| Data Preprocessing | Clean, tokenize, remove stopwords, lemmatize |\n",
    "| Feature Extraction | Convert text to TF-IDF vectors               |\n",
    "| Model Training     | Train classifier (Logistic Regression)       |\n",
    "| Evaluation         | Assess accuracy and metrics                  |\n",
    "| Prediction         | Use pipeline to predict new samples          |\n",
    "\n",
    "---\n",
    "\n",
    "This workflow can be extended with advanced preprocessing (POS tagging, embeddings), models (SVM, Random Forest, Transformers), and hyperparameter tuning for production-grade systems.\n",
    "\n",
    "Would you like a walkthrough with advanced embedding techniques or deep learning models next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47770af2",
   "metadata": {},
   "source": [
    "Certainly, Suraj. Here's an alternate end-to-end NLP model training example for **spam email classification** using **Word2Vec embeddings** combined with a **Random Forest classifier** ‚Äî showcasing how to integrate word embeddings into the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# End-to-End NLP Pipeline: Spam Classification Using Word2Vec + Random Forest\n",
    "\n",
    "### 1. **Sample Data Preparation**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Congratulations! You've won a free ticket.\",\n",
    "        \"Hi, can we reschedule the meeting?\",\n",
    "        \"Get cheap meds now!!!\",\n",
    "        \"Please find the attached report.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0]  # 1: Spam, 0: Not spam\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Preprocessing Function**\n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['text'].apply(preprocess)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Load Pre-trained Word2Vec & Compute Average Embeddings**\n",
    "\n",
    "```python\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def avg_word2vec(tokens, model, vector_size=300):\n",
    "    valid_words = [model[word] for word in tokens if word in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(valid_words, axis=0)\n",
    "\n",
    "df['avg_vec'] = df['tokens'].apply(lambda x: avg_word2vec(x, model))\n",
    "X = np.vstack(df['avg_vec'].values)\n",
    "y = df['label'].values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Train-Test Split**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Model Training: Random Forest**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Evaluation**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Predict on New Example**\n",
    "\n",
    "```python\n",
    "def predict_spam(text):\n",
    "    tokens = preprocess(text)\n",
    "    vector = avg_word2vec(tokens, model).reshape(1, -1)\n",
    "    pred = rf.predict(vector)\n",
    "    return \"Spam\" if pred[0] == 1 else \"Not Spam\"\n",
    "\n",
    "print(predict_spam(\"Win a brand new car now!\"))\n",
    "print(predict_spam(\"Are we meeting tomorrow?\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "| Step               | Description                                                  |\n",
    "| ------------------ | ------------------------------------------------------------ |\n",
    "| Preprocessing      | Text cleaning, tokenization, stopword removal, lemmatization |\n",
    "| Feature Extraction | Compute average Word2Vec embeddings                          |\n",
    "| Model Training     | Random Forest classifier                                     |\n",
    "| Evaluation         | Accuracy and classification report                           |\n",
    "| Prediction         | Predict on new raw text using the pipeline                   |\n",
    "\n",
    "---\n",
    "\n",
    "This approach leverages the semantic richness of Word2Vec with a robust ensemble classifier, illustrating a practical workflow beyond simple TF-IDF.\n",
    "\n",
    "Let me know if you want to explore deep learning models like LSTM or Transformer-based pipelines next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2375e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
