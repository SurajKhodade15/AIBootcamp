{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a57610",
   "metadata": {},
   "source": [
    "\n",
    "## **Steps in Self-Attention Layer**\n",
    "\n",
    "1. **Input Representation**\n",
    "\n",
    "   * Start with input embeddings $X \\in \\mathbb{R}^{n \\times d_{model}}$, where:\n",
    "\n",
    "     * $n$ = sequence length\n",
    "     * $d_{model}$ = embedding dimension\n",
    "\n",
    "2. **Linear Transformations**\n",
    "\n",
    "   * Project the input into **Query (Q)**, **Key (K)**, and **Value (V)** vectors using learnable weight matrices:\n",
    "\n",
    "     $$\n",
    "     Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V\n",
    "     $$\n",
    "\n",
    "     where $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$.\n",
    "\n",
    "3. **Similarity Scores (Dot Product)**\n",
    "\n",
    "   * Compute similarity between queries and keys:\n",
    "\n",
    "     $$\n",
    "     \\text{scores} = QK^T\n",
    "     $$\n",
    "\n",
    "     This results in a $n \\times n$ matrix representing the relevance between tokens.\n",
    "\n",
    "4. **Scaling**\n",
    "\n",
    "   * Scale the scores to prevent large values (which can cause gradient instability):\n",
    "\n",
    "     $$\n",
    "     \\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}}\n",
    "     $$\n",
    "\n",
    "5. **Softmax Normalization**\n",
    "\n",
    "   * Apply softmax to convert scaled scores into attention weights:\n",
    "\n",
    "     $$\n",
    "     \\alpha_{ij} = \\frac{\\exp(\\text{scaled\\_scores}_{ij})}{\\sum_{k=1}^n \\exp(\\text{scaled\\_scores}_{ik})}\n",
    "     $$\n",
    "\n",
    "     This ensures weights sum to 1 for each query.\n",
    "\n",
    "6. **Weighted Sum of Values**\n",
    "\n",
    "   * Multiply the attention weights with the value vectors:\n",
    "\n",
    "     $$\n",
    "     \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "     $$\n",
    "\n",
    "7. **Output**\n",
    "\n",
    "   * The result is a context-aware representation of each token, capturing relationships across the sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc57c7",
   "metadata": {},
   "source": [
    "\n",
    "## **Steps in Self-Attention Layer**\n",
    "\n",
    "1. **Input Representation**\n",
    "\n",
    "   * Start with input embeddings $X \\in \\mathbb{R}^{n \\times d_{model}}$, where:\n",
    "\n",
    "     * $n$ = sequence length\n",
    "     * $d_{model}$ = embedding dimension\n",
    "\n",
    "2. **Linear Transformations**\n",
    "\n",
    "   * Project the input into **Query (Q)**, **Key (K)**, and **Value (V)** vectors using learnable weight matrices:\n",
    "\n",
    "     $$\n",
    "     Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V\n",
    "     $$\n",
    "\n",
    "     where $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$.\n",
    "\n",
    "3. **Similarity Scores (Dot Product)**\n",
    "\n",
    "   * Compute similarity between queries and keys:\n",
    "\n",
    "     $$\n",
    "     \\text{scores} = QK^T\n",
    "     $$\n",
    "\n",
    "     This results in a $n \\times n$ matrix representing the relevance between tokens.\n",
    "\n",
    "4. **Scaling**\n",
    "\n",
    "   * Scale the scores to prevent large values (which can cause gradient instability):\n",
    "\n",
    "     $$\n",
    "     \\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}}\n",
    "     $$\n",
    "\n",
    "5. **Softmax Normalization**\n",
    "\n",
    "   * Apply softmax to convert scaled scores into attention weights:\n",
    "\n",
    "     $$\n",
    "     \\alpha_{ij} = \\frac{\\exp(\\text{scaled\\_scores}_{ij})}{\\sum_{k=1}^n \\exp(\\text{scaled\\_scores}_{ik})}\n",
    "     $$\n",
    "\n",
    "     This ensures weights sum to 1 for each query.\n",
    "\n",
    "6. **Weighted Sum of Values**\n",
    "\n",
    "   * Multiply the attention weights with the value vectors:\n",
    "\n",
    "     $$\n",
    "     \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "     $$\n",
    "\n",
    "7. **Output**\n",
    "\n",
    "   * The result is a context-aware representation of each token, capturing relationships across the sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5dfef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
