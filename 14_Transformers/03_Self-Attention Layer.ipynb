{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a57610",
   "metadata": {},
   "source": [
    "\n",
    "## **Steps in Self-Attention Layer**\n",
    "\n",
    "1. **Input Representation**\n",
    "\n",
    "   * Start with input embeddings $X \\in \\mathbb{R}^{n \\times d_{model}}$, where:\n",
    "\n",
    "     * $n$ = sequence length\n",
    "     * $d_{model}$ = embedding dimension\n",
    "\n",
    "2. **Linear Transformations**\n",
    "\n",
    "   * Project the input into **Query (Q)**, **Key (K)**, and **Value (V)** vectors using learnable weight matrices:\n",
    "\n",
    "     $$\n",
    "     Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V\n",
    "     $$\n",
    "\n",
    "     where $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$.\n",
    "\n",
    "3. **Similarity Scores (Dot Product)**\n",
    "\n",
    "   * Compute similarity between queries and keys:\n",
    "\n",
    "     $$\n",
    "     \\text{scores} = QK^T\n",
    "     $$\n",
    "\n",
    "     This results in a $n \\times n$ matrix representing the relevance between tokens.\n",
    "\n",
    "4. **Scaling**\n",
    "\n",
    "   * Scale the scores to prevent large values (which can cause gradient instability):\n",
    "\n",
    "     $$\n",
    "     \\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}}\n",
    "     $$\n",
    "\n",
    "5. **Softmax Normalization**\n",
    "\n",
    "   * Apply softmax to convert scaled scores into attention weights:\n",
    "\n",
    "     $$\n",
    "     \\alpha_{ij} = \\frac{\\exp(\\text{scaled\\_scores}_{ij})}{\\sum_{k=1}^n \\exp(\\text{scaled\\_scores}_{ik})}\n",
    "     $$\n",
    "\n",
    "     This ensures weights sum to 1 for each query.\n",
    "\n",
    "6. **Weighted Sum of Values**\n",
    "\n",
    "   * Multiply the attention weights with the value vectors:\n",
    "\n",
    "     $$\n",
    "     \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "     $$\n",
    "\n",
    "7. **Output**\n",
    "\n",
    "   * The result is a context-aware representation of each token, capturing relationships across the sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc57c7",
   "metadata": {},
   "source": [
    "\n",
    "## **Steps in Self-Attention Layer**\n",
    "\n",
    "1. **Input Representation**\n",
    "\n",
    "   * Start with input embeddings $X \\in \\mathbb{R}^{n \\times d_{model}}$, where:\n",
    "\n",
    "     * $n$ = sequence length\n",
    "     * $d_{model}$ = embedding dimension\n",
    "\n",
    "2. **Linear Transformations**\n",
    "\n",
    "   * Project the input into **Query (Q)**, **Key (K)**, and **Value (V)** vectors using learnable weight matrices:\n",
    "\n",
    "     $$\n",
    "     Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V\n",
    "     $$\n",
    "\n",
    "     where $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$.\n",
    "\n",
    "3. **Similarity Scores (Dot Product)**\n",
    "\n",
    "   * Compute similarity between queries and keys:\n",
    "\n",
    "     $$\n",
    "     \\text{scores} = QK^T\n",
    "     $$\n",
    "\n",
    "     This results in a $n \\times n$ matrix representing the relevance between tokens.\n",
    "\n",
    "4. **Scaling**\n",
    "\n",
    "   * Scale the scores to prevent large values (which can cause gradient instability):\n",
    "\n",
    "     $$\n",
    "     \\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}}\n",
    "     $$\n",
    "\n",
    "5. **Softmax Normalization**\n",
    "\n",
    "   * Apply softmax to convert scaled scores into attention weights:\n",
    "\n",
    "     $$\n",
    "     \\alpha_{ij} = \\frac{\\exp(\\text{scaled\\_scores}_{ij})}{\\sum_{k=1}^n \\exp(\\text{scaled\\_scores}_{ik})}\n",
    "     $$\n",
    "\n",
    "     This ensures weights sum to 1 for each query.\n",
    "\n",
    "6. **Weighted Sum of Values**\n",
    "\n",
    "   * Multiply the attention weights with the value vectors:\n",
    "\n",
    "     $$\n",
    "     \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "     $$\n",
    "\n",
    "7. **Output**\n",
    "\n",
    "   * The result is a context-aware representation of each token, capturing relationships across the sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5dfef",
   "metadata": {},
   "source": [
    "| Question                                                                          | Answer                                                                                                                                                                 |\n",
    "| --------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Q1. What is the main purpose of the self-attention mechanism in Transformers?** | To allow each token in the input to focus on other tokens, capturing context and dependencies regardless of their position in the sequence.                            |\n",
    "| **Q2. Why do we scale the dot product by √d\\_k?**                                 | Scaling prevents large dot-product values when the dimensionality is high, which could lead to very small gradients after softmax, slowing down training.              |\n",
    "| **Q3. Difference between Self-Attention and Cross-Attention?**                    | Self-Attention uses Q, K, V from the same sequence, while Cross-Attention uses Q from one sequence (e.g., decoder) and K, V from another (e.g., encoder).              |\n",
    "| **Q4. How does Multi-Head Attention relate to Self-Attention?**                   | Multi-Head Attention performs multiple self-attentions in parallel with different learned projections, allowing the model to capture different types of relationships. |\n",
    "| **Q5. Computational complexity of Self-Attention?**                               | O(n²·d) where n is sequence length and d is embedding dimension — due to QKᵀ computation for all token pairs.                                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a57d93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
