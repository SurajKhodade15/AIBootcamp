{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a3f090",
   "metadata": {},
   "source": [
    "\n",
    "## **Multi-Head Attention in Transformers**\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "Multi-Head Attention (MHA) is a core component of Transformer architectures that allows the model to attend to different positions in the sequence representation from multiple perspectives (or “heads”). Instead of performing a single self-attention operation, MHA performs several in parallel and concatenates the results, enabling richer context representation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in Multi-Head Attention**\n",
    "\n",
    "1. **Input Projection**\n",
    "\n",
    "   * The input embedding (or hidden state) is projected into three distinct spaces:\n",
    "\n",
    "     * **Query (Q)**\n",
    "     * **Key (K)**\n",
    "     * **Value (V)**\n",
    "   * These projections are done *separately for each head*.\n",
    "\n",
    "2. **Scaled Dot-Product Attention (per head)**\n",
    "\n",
    "   * For each head, compute attention weights using:\n",
    "\n",
    "     $$\n",
    "     \\text{Attention}(Q, K, V) = \\text{Softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "     $$\n",
    "\n",
    "     * $d_k$ is the dimension of each key vector.\n",
    "     * Scaling avoids large dot products that could cause small gradient updates.\n",
    "\n",
    "3. **Parallel Attention**\n",
    "\n",
    "   * Each head learns different relationships (e.g., syntax, semantics) independently.\n",
    "\n",
    "4. **Concatenation**\n",
    "\n",
    "   * The outputs of all heads are concatenated.\n",
    "\n",
    "5. **Final Linear Projection**\n",
    "\n",
    "   * A final linear layer merges the concatenated heads into a single tensor.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Representation**\n",
    "\n",
    "If there are $h$ heads:\n",
    "\n",
    "$$\n",
    "\\text{MHA}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "Suppose:\n",
    "\n",
    "* Input embedding size = **8**\n",
    "* Number of heads = **2**\n",
    "* Each head dimension = **4**\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "1. Project embeddings into Q, K, V for head 1 and head 2 (each of size 4).\n",
    "2. Compute scaled dot-product attention separately for each head.\n",
    "3. Concatenate the two attention outputs (size 8).\n",
    "4. Apply a linear transformation to mix head outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "* Captures **multiple types of relationships** in parallel.\n",
    "* More expressive than single-head attention.\n",
    "* Helps in modeling **long-range dependencies** better.\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "* Increased computational cost.\n",
    "* More memory usage.\n",
    "* Potential redundancy if heads learn similar patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interview Q\\&A**\n",
    "\n",
    "| **Question**                                       | **Answer**                                                                                                                                  |\n",
    "| -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Why do we use multiple heads instead of one?**   | Multiple heads allow the model to attend to different types of relationships in the data simultaneously, leading to richer representations. |\n",
    "| **Why is the dot product scaled by $\\sqrt{d_k}$?** | To prevent very large dot product values which can push the softmax into regions with extremely small gradients.                            |\n",
    "| **Do all heads have the same parameters?**         | No, each head has its own learnable projection matrices $W_i^Q, W_i^K, W_i^V$.                                                              |\n",
    "| **What happens after concatenating the heads?**    | The concatenated tensor is passed through a final linear transformation $W^O$ to mix the information from all heads.                        |\n",
    "| **Can we have different head sizes in MHA?**       | Typically, all heads have the same dimension for implementation efficiency, but in theory, variable sizes are possible.                     |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
