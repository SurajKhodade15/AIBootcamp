{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64da564f",
   "metadata": {},
   "source": [
    "\n",
    "# **Architecture of Transformer**\n",
    "\n",
    "## **1. Overview**\n",
    "\n",
    "The Transformer architecture, introduced in the paper *\"Attention Is All You Need\"* (Vaswani et al., 2017), is a neural network design that completely replaces recurrence and convolution with the **self-attention mechanism**.\n",
    "It is widely used in **Natural Language Processing (NLP)**, especially in models such as BERT, GPT, and T5.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Core Components**\n",
    "\n",
    "A standard Transformer consists of **two main blocks**:\n",
    "\n",
    "* **Encoder**: Processes the input sequence and creates contextual representations.\n",
    "* **Decoder**: Generates the output sequence, attending to encoder outputs and previously generated tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Detailed Architecture**\n",
    "\n",
    "### **A. Encoder**\n",
    "\n",
    "Each encoder layer contains:\n",
    "\n",
    "1. **Input Embedding + Positional Encoding** – Adds positional context to embeddings.\n",
    "2. **Multi-Head Self-Attention Layer** – Computes relationships between all tokens in the input.\n",
    "3. **Add & Norm** – Residual connection followed by layer normalization.\n",
    "4. **Feed-Forward Neural Network (FFN)** – Two fully connected layers with non-linear activation (ReLU/GELU).\n",
    "5. **Add & Norm** – Another residual connection and normalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Decoder**\n",
    "\n",
    "Each decoder layer contains:\n",
    "\n",
    "1. **Output Embedding + Positional Encoding**\n",
    "2. **Masked Multi-Head Self-Attention** – Ensures the model doesn’t peek ahead at future tokens.\n",
    "3. **Add & Norm**\n",
    "4. **Encoder–Decoder Attention** – Allows the decoder to focus on relevant encoder outputs.\n",
    "5. **Add & Norm**\n",
    "6. **Feed-Forward Neural Network**\n",
    "7. **Add & Norm**\n",
    "8. **Linear & Softmax Layer** – Generates the probability distribution over the vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Transformer Block Flow**\n",
    "\n",
    "```plaintext\n",
    "Input Sequence\n",
    "   ↓\n",
    "Embedding + Positional Encoding\n",
    "   ↓\n",
    "[Encoder Layers × N]\n",
    "   ↓\n",
    "Encoded Representations\n",
    "   ↓\n",
    "[Decoder Layers × N]\n",
    "   ↓\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Key Features**\n",
    "\n",
    "* **No Recurrence** – Processes sequences in parallel.\n",
    "* **Self-Attention** – Captures long-range dependencies efficiently.\n",
    "* **Positional Encoding** – Adds sequential order information.\n",
    "* **Scalability** – Works well for large datasets and models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911e4a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
