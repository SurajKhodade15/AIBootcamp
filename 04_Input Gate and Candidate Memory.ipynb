{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5340b14",
   "metadata": {},
   "source": [
    "Certainly. Here is the detailed explanation of the **Input Gate and Candidate Memory** in LSTM, formatted for seamless integration into your Jupyter Notebook markdown cells:\n",
    "\n",
    "---\n",
    "\n",
    "## Input Gate and Candidate Memory in LSTM\n",
    "\n",
    "| Aspect                          | Details                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Input Gate Definition**       | The input gate controls how much new information from the current input and previous hidden state should be added to the cell state. It selectively updates the memory with relevant information.                                                                                                                                                                                                      |\n",
    "| **Candidate Memory Definition** | Candidate memory (also called candidate cell state) represents potential new values created from the current input and previous hidden state, which may be added to the cell state after filtering by the input gate.                                                                                                                                                                                  |\n",
    "| **Mathematical Formulas**       | - **Input Gate Activation:**  $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$ <br> - **Candidate Memory:** $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$                                                                                                                                                                                                                                        |\n",
    "| **Explanation**                 | - The input gate $i_t$ uses a sigmoid function to output values between 0 and 1, determining how much of the candidate memory to add.<br> - The candidate memory $\\tilde{C}_t$ uses a tanh activation to create new candidate values scaled between $-1$ and $1$.<br> - The element-wise product $i_t \\odot \\tilde{C}_t$ represents the filtered new information that will be added to the cell state. |\n",
    "| **Role in LSTM**                | Enables the LSTM cell to **selectively update** its memory, integrating new relevant information while ignoring irrelevant data.                                                                                                                                                                                                                                                                       |\n",
    "| **Use Cases**                   | - When new important information arrives in sequences such as language or time series, the input gate decides how much to remember.<br> - Helps model dynamic changes in context or pattern over time.                                                                                                                                                                                                 |\n",
    "| **Interview Q\\&A**              | **Q:** Why does the candidate memory use tanh activation? <br> **A:** Tanh outputs values in $[-1, 1]$, allowing candidate memory to represent positive and negative influences on the cell state, supporting richer memory updates.                                                                                                                                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "### Python Example â€“ Input Gate and Candidate Memory\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample input vectors\n",
    "x_t = np.array([0.6, -0.3])           # Current input\n",
    "h_t_minus_1 = np.array([0.1, 0.5])   # Previous hidden state\n",
    "\n",
    "# Weight matrices and biases (random initialization for demonstration)\n",
    "W_i = np.random.randn(4, 4)  # Weight matrix for input gate\n",
    "b_i = np.random.randn(4)     # Bias for input gate\n",
    "W_C = np.random.randn(4, 4)  # Weight matrix for candidate memory\n",
    "b_C = np.random.randn(4)     # Bias for candidate memory\n",
    "\n",
    "# Concatenate previous hidden state and current input\n",
    "concat_input = np.concatenate((h_t_minus_1, x_t))\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "# Input gate computation\n",
    "i_t = sigmoid(np.dot(concat_input, W_i) + b_i)\n",
    "\n",
    "# Candidate memory computation\n",
    "C_tilde = tanh(np.dot(concat_input, W_C) + b_C)\n",
    "\n",
    "print(\"Input Gate Output:\", i_t)\n",
    "print(\"Candidate Memory:\", C_tilde)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to continue with the **Cell State Update** next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ee460",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
