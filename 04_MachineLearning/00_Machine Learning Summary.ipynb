{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d514d0d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f7ea5",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning Introduction \n",
    "\n",
    "Machine learning is a subset of artificial intelligence (AI) focused on designing algorithms that enable computers to learn patterns and make decisions from data, without being directly programmed for every possible scenario. Unlike traditional programming (where explicit rules are coded), machine learning algorithms develop their own logic based on examples and feedback, improving performance as they are exposed to more data.\n",
    "\n",
    "**Types of Machine Learning**\n",
    "\n",
    "| Type                  | Description                                                                                   | Examples                        |\n",
    "|-----------------------|-----------------------------------------------------------------------------------------------|----------------------------------|\n",
    "| Supervised Learning   | Learns from labeled data to predict outcomes.                                                 | Email spam detection, regression |\n",
    "| Unsupervised Learning | Finds patterns and groupings in unlabeled data.                                               | Customer segmentation, clustering|\n",
    "| Reinforcement Learning| Learns by trial and error, receiving rewards/penalties from its environment.                  | Game playing, robotics           |\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "- **Data:** The dataset used for training, crucial for effective learning.\n",
    "- **Model:** The algorithm or mathematical structure that learns from data (e.g., neural network, decision tree).\n",
    "- **Training:** The process where the model 'learns' patterns and adjusts its internal settings for best predictions.\n",
    "\n",
    "**Common Interview Questions**\n",
    "\n",
    "| Question                                                          | Key Point/Short Answer                                                                    |\n",
    "|-------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
    "| What is machine learning?                                          | The field where computer systems learn from data without explicit programming.             |\n",
    "| Types of machine learning?                                         | Supervised, Unsupervised, Reinforcement learning (see above table).                       |\n",
    "| Difference from traditional programming?                           | ML creates logic from data; traditional uses explicit rules coded by programmers.          |\n",
    "| Real-world applications?                                           | Spam filtering, recommendations, facial/speech recognition, fraud detection, self-driving.|\n",
    "| What are features and labels?                                      | Features are input variables; labels are the target outcomes for prediction tasks.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced90c6",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning\n",
    "\n",
    "Supervised machine learning is a type of machine learning where algorithms are trained using labeled data, meaning each input has a corresponding correct output. The model learns the relationship between features (inputs) and labels (outputs) by analyzing these pairs. Its objective is to predict accurate outcomes for new, unseen data by generalizing from the training data patterns.\n",
    "\n",
    "**Types of Supervised Learning Tasks**\n",
    "- **Classification:** Predicting categorical labels (e.g., spam vs. non-spam emails).\n",
    "- **Regression:** Predicting continuous numerical values (e.g., house price prediction).\n",
    "\n",
    "**Key Process Steps**\n",
    "- Train the model on labeled data (features and labels).\n",
    "- Predict outputs and compare them with actual labels.\n",
    "- Adjust model parameters to minimize errors.\n",
    "- Evaluate performance on test data.\n",
    "\n",
    "| **Aspect**         | **Description**                                         | **Example Tasks**                |\n",
    "|--------------------|--------------------------------------------------------|----------------------------------|\n",
    "| Input Data         | Labeled data with features and corresponding labels     | Emails + spam/not spam labels    |\n",
    "| Goal               | Learn mapping from inputs to outputs                   | Classification, regression       |\n",
    "| Common Tasks       | Classification and regression                          | Spam detection, price prediction |\n",
    "| Evaluation         | Measure accuracy or error on test data                 | Accuracy, RMSE                   |\n",
    "\n",
    "**Common Interview Questions**\n",
    "\n",
    "| **Question**                             | **Key Point/Short Answer**                                                                |\n",
    "|------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| What is supervised learning?             | Learning from labeled data to predict future outcomes.                                   |\n",
    "| Difference from unsupervised learning?   | Supervised uses labeled data; unsupervised finds patterns in unlabeled data.             |\n",
    "| Classification vs. regression?           | Classification predicts categories; regression predicts continuous values.               |\n",
    "| What is overfitting and how to prevent it?| Overfitting occurs when the model learns noise; prevented by cross-validation, regularization, pruning. |\n",
    "| Bias-variance tradeoff?                  | The balance between underfitting (high bias) and overfitting (high variance).            |\n",
    "| Evaluation metrics?                      | Accuracy, Precision, Recall, F1 Score for classification; RMSE, MAE for regression.      |\n",
    "| Examples of supervised algorithms?       | Linear regression, logistic regression, decision trees, random forest, SVM, k-NN, neural networks. |\n",
    "| What is cross-validation?                | A technique to assess generalization by splitting data into multiple train/test sets.    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c106a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260f177f",
   "metadata": {},
   "source": [
    "Here is the previous content with sections that can be logically represented in **tabular format** converted into tables:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ **Theory: Simple Linear Regression**\n",
    "\n",
    "**Definition:**\n",
    "Simple Linear Regression is a supervised learning algorithm used to predict a **continuous** target variable $y$ based on a **single** independent variable $x$. It assumes a **linear** relationship between $x$ and $y$ and fits a straight line to the data.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Model Equation**\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "| Symbol     | Meaning                                        |\n",
    "| ---------- | ---------------------------------------------- |\n",
    "| $y$        | Dependent (target) variable                    |\n",
    "| $x$        | Independent (feature) variable                 |\n",
    "| $\\beta_0$  | Intercept (value of $y$ when $x=0$)            |\n",
    "| $\\beta_1$  | Slope (change in $y$ for a unit change in $x$) |\n",
    "| $\\epsilon$ | Error term                                     |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **How the Model Learns**\n",
    "\n",
    "| Step | Description                                                        |\n",
    "| ---- | ------------------------------------------------------------------ |\n",
    "| 1    | Estimates $\\beta_0$ and $\\beta_1$ to minimize prediction errors    |\n",
    "| 2    | Uses **Mean Squared Error (MSE)** as the cost function             |\n",
    "| 3    | Applies **Ordinary Least Squares (OLS)** to find the best-fit line |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Assumptions of Linear Regression**\n",
    "\n",
    "| Assumption       | Description                                |\n",
    "| ---------------- | ------------------------------------------ |\n",
    "| Linearity        | Relationship between $x$ and $y$ is linear |\n",
    "| Independence     | Observations are independent               |\n",
    "| Homoscedasticity | Residuals have constant variance           |\n",
    "| Normality        | Residuals are normally distributed         |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                           | Disadvantages                                |\n",
    "| ------------------------------------ | -------------------------------------------- |\n",
    "| Easy to implement and interpret      | Assumes linearity; fails on non-linear data  |\n",
    "| Computationally efficient            | Sensitive to outliers                        |\n",
    "| Provides a baseline for other models | Poor performance if assumptions are violated |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Interview Insights**\n",
    "\n",
    "### âœ… **Basic Level**\n",
    "\n",
    "| Question                          | Answer                                                                                                           |\n",
    "| --------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| What is simple linear regression? | Itâ€™s an algorithm that models a linear relationship between one independent variable and one dependent variable. |\n",
    "| Give a real-world example of SLR. | Predicting house price based on its size.                                                                        |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Intermediate Level**\n",
    "\n",
    "| Question                                              | Answer                                                                                              |\n",
    "| ----------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| What is the cost function used in linear regression?  | Mean Squared Error (MSE).                                                                           |\n",
    "| How are parameters $\\beta_0$ and $\\beta_1$ estimated? | Using the Ordinary Least Squares (OLS) method, which minimizes the sum of squared residuals.        |\n",
    "| What is $R^2$ in linear regression?                   | A metric that explains the proportion of variance in the dependent variable explained by the model. |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Advanced Level**\n",
    "\n",
    "| Question                                                                            | Answer                                                                                                             |\n",
    "| ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
    "| Explain the assumptions of linear regression and what happens if they are violated. | Violations may lead to biased or inefficient estimates; e.g., heteroscedasticity affects standard errors.          |\n",
    "| How would you detect and handle outliers in linear regression?                      | Use residual plots, leverage scores, Cookâ€™s distance; handle by removing or applying robust regression techniques. |\n",
    "| Why is gradient descent not commonly used for simple linear regression?             | Because OLS has an analytical solution, making it computationally simpler.                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cba6f7",
   "metadata": {},
   "source": [
    "## Cost Functions in Machine Learning\n",
    "\n",
    "Cost functions (also known as loss or objective functions) quantify how well a machine learning model is performing by measuring the difference between predicted values and actual values. The ultimate goal of training a model is to minimize the cost function, leading to better accuracy.\n",
    "\n",
    "### Common Cost Functions and Their Use Cases\n",
    "\n",
    "| **Cost Function**            | **Type**          | **Formula (for single sample)**                                               | **Use Case / Notes**                                                                                            |\n",
    "|-----------------------------|-------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| Mean Squared Error (MSE)     | Regression        | $$\\frac{1}{2} (y_\\text{pred} - y_\\text{true})^2$$                          | Most popular for regression; penalizes large errors heavily; differentiable and convex                        |\n",
    "| Mean Absolute Error (MAE)    | Regression        | $$|y_\\text{pred} - y_\\text{true}|$$                                        | Robust to outliers; less sensitive than MSE; not differentiable at zero                                        |\n",
    "| Root Mean Squared Error (RMSE) | Regression      | $$\\sqrt{\\frac{1}{m} \\sum (y_\\text{pred} - y_\\text{true})^2}$$             | Square root of MSE, measuring error in original units                                                          |\n",
    "| Mean Absolute Percentage Error (MAPE) | Regression | $$\\frac{1}{m} \\sum \\left| \\frac{y_\\text{true} - y_\\text{pred}}{y_\\text{true}} \\right|$$ | Measures prediction accuracy in percentage terms                                                               |\n",
    "| Huber Loss                   | Regression        | Piecewise: quadratic if error < Î´, linear otherwise                         | Combines advantages of MSE and MAE; less sensitive to outliers                                                 |\n",
    "| Binary Cross-Entropy         | Binary Classification | $$-[y \\log(p) + (1 - y) \\log(1 - p)]$$                                    | Measures error between predicted probabilities and actual classes                                              |\n",
    "| Categorical Cross-Entropy    | Multi-class Classification | $$-\\sum_k y_k \\log(p_k)$$                                                  | Extends binary cross-entropy to multi-class problems                                                            |\n",
    "| Hinge Loss                   | Classification    | $$\\max(0, 1 - y \\cdot f(x))$$                                              | Used by support vector machines; tries to maximize margin between classes                                      |\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Regression Tasks:** MSE, MAE, RMSE, MAPE, and Huber Loss are common. MSE is the most widely used because it penalizes large errors, but MAE and Huber Loss are more robust to outliers.\n",
    "- **Classification Tasks:** Cross-Entropy Loss (binary or categorical) is prevalent because it works well with probabilistic outputs from classifiers. Hinge Loss is used with support vector machines.\n",
    "- The choice depends on the problem type, data distribution, robustness needs, and model framework.\n",
    "\n",
    "### Role in Training\n",
    "\n",
    "- The cost function outputs a scalar error value representing model performance.\n",
    "- Optimization algorithms minimize this cost by adjusting model parameters.\n",
    "- Well-chosen cost functions lead to faster convergence and better generalization.\n",
    "\n",
    "This overview of cost functions can be directly incorporated into your Jupyter notebook for study or reference.\n",
    "\n",
    "[1] https://www.analyticssteps.com/blogs/7-types-cost-functions-machine-learning\n",
    "[2] https://intellipaat.com/blog/cost-function-in-machine-learning/\n",
    "[3] https://www.alooba.com/skills/concepts/machine-learning-11/cost-functions/\n",
    "[4] https://www.analytixlabs.co.in/blog/cost-function-in-machine-learning/\n",
    "[5] https://wisdomplexus.com/blogs/cost-function-in-machine-learning-meaning-types-and-importance/\n",
    "[6] https://www.numberanalytics.com/blog/ultimate-guide-cost-function-machine-learning\n",
    "[7] https://www.geeksforgeeks.org/machine-learning/ml-cost-function-in-logistic-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2d59e",
   "metadata": {},
   "source": [
    "## ðŸ“˜ **Theory: Multiple Linear Regression**\n",
    "\n",
    "**Definition:**\n",
    "Multiple Linear Regression (MLR) is an extension of simple linear regression where the target variable $y$ depends on **two or more independent variables** $x_1, x_2, ..., x_n$.\n",
    "\n",
    "* It models the relationship between multiple predictors and a continuous outcome.\n",
    "* The model fits a hyperplane (instead of a line) in an n-dimensional feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Model Equation**\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "| Symbol               | Meaning                                                     |\n",
    "| -------------------- | ----------------------------------------------------------- |\n",
    "| $y$                  | Dependent (target) variable                                 |\n",
    "| $x_1, x_2, ..., x_n$ | Independent (feature) variables                             |\n",
    "| $\\beta_0$            | Intercept (value of $y$ when all $x_i=0$)                   |\n",
    "| $\\beta_i$            | Coefficient representing the effect of feature $x_i$ on $y$ |\n",
    "| $\\epsilon$           | Error term capturing noise in the data                      |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **How the Model Learns**\n",
    "\n",
    "| Step | Description                                                                                    |\n",
    "| ---- | ---------------------------------------------------------------------------------------------- |\n",
    "| 1    | Estimate coefficients $\\beta_0, \\beta_1, ..., \\beta_n$ using **Ordinary Least Squares (OLS)**. |\n",
    "| 2    | Minimize the **Mean Squared Error (MSE)** cost function.                                       |\n",
    "| 3    | The fitted model predicts $y$ by summing contributions from all features.                      |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Assumptions of Multiple Linear Regression**\n",
    "\n",
    "| Assumption           | Description                                           |\n",
    "| -------------------- | ----------------------------------------------------- |\n",
    "| Linearity            | Relationship between predictors and target is linear. |\n",
    "| Independence         | Observations are independent.                         |\n",
    "| Homoscedasticity     | Residuals have constant variance.                     |\n",
    "| Normality            | Residuals are normally distributed.                   |\n",
    "| No Multicollinearity | Predictors are not highly correlated with each other. |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                           | Disadvantages                                          |\n",
    "| ---------------------------------------------------- | ------------------------------------------------------ |\n",
    "| Models relationships with multiple factors           | Sensitive to multicollinearity                         |\n",
    "| Easy to interpret (coefficients show feature impact) | Assumes linearity, may not capture non-linear patterns |\n",
    "| Efficient and widely used                            | Outliers can distort the model                         |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Interview Insights**\n",
    "\n",
    "### âœ… **Basic Level**\n",
    "\n",
    "| Question                            | Answer                                                                                                           |\n",
    "| ----------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| What is multiple linear regression? | Itâ€™s a regression technique that predicts a continuous target variable using more than one independent variable. |\n",
    "| Give an example of MLR.             | Predicting house prices using size, location, and number of rooms.                                               |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Intermediate Level**\n",
    "\n",
    "| Question                                                  | Answer                                                                                   |\n",
    "| --------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| What cost function is used in multiple linear regression? | Mean Squared Error (MSE).                                                                |\n",
    "| What is multicollinearity?                                | When independent variables are highly correlated, making coefficient estimates unstable. |\n",
    "| How can you detect multicollinearity?                     | Using Variance Inflation Factor (VIF) or correlation matrices.                           |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Advanced Level**\n",
    "\n",
    "| Question                                                 | Answer                                                                                                                 |\n",
    "| -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| How do you handle multicollinearity?                     | Remove correlated variables, apply dimensionality reduction (e.g., PCA), or use regularization (Ridge/Lasso).          |\n",
    "| What metrics evaluate the modelâ€™s performance?           | $R^2$, Adjusted $R^2$, RMSE, MAE.                                                                                      |\n",
    "| What is the difference between $R^2$ and Adjusted $R^2$? | Adjusted $R^2$ penalizes the addition of irrelevant variables, giving a more reliable measure for multiple predictors. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b2a0b",
   "metadata": {},
   "source": [
    "## ðŸ“˜ **Theory: Performance Metrics in Machine Learning**\n",
    "\n",
    "**Definition:**\n",
    "Performance metrics are quantitative measures used to evaluate how well a machine learning model performs on unseen data.\n",
    "\n",
    "* The choice of metric depends on the type of problem: **Regression** or **Classification**.\n",
    "* Proper evaluation ensures the model generalizes well and is not overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Performance Metrics for Regression**\n",
    "\n",
    "| Metric                             | Formula                                   | Range          | Interpretation                                                  |               |                                      |\n",
    "| ---------------------------------- | ----------------------------------------- | -------------- | --------------------------------------------------------------- | ------------- | ------------------------------------ |\n",
    "| **Mean Squared Error (MSE)**       | $\\frac{1}{n} \\sum (\\hat{y} - y)^2$        | $[0, \\infty)$  | Lower is better; penalizes large errors heavily.                |               |                                      |\n",
    "| **Root Mean Squared Error (RMSE)** | $\\sqrt{\\frac{1}{n} \\sum (\\hat{y} - y)^2}$ | $[0, \\infty)$  | Same as MSE but in original units of $y$.                       |               |                                      |\n",
    "| **Mean Absolute Error (MAE)**      | (\\frac{1}{n} \\sum                         | \\hat{y} - y    | )                                                               | $[0, \\infty)$ | Less sensitive to outliers than MSE. |\n",
    "| **R-Squared ($R^2$)**              | $1 - \\frac{SS_{res}}{SS_{tot}}$           | $(-\\infty, 1]$ | Proportion of variance explained; closer to 1 is better.        |               |                                      |\n",
    "| **Adjusted $R^2$**                 | $1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}$  | $(-\\infty, 1]$ | Adjusts $R^2$ for number of predictors to avoid overestimation. |               |                                      |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Performance Metrics for Classification**\n",
    "\n",
    "| Metric                   | Formula                                                             | Range         | Interpretation                                                 |\n",
    "| ------------------------ | ------------------------------------------------------------------- | ------------- | -------------------------------------------------------------- |\n",
    "| **Accuracy**             | $\\frac{TP + TN}{TP + TN + FP + FN}$                                 | $[0, 1]$      | Percentage of correctly classified instances.                  |\n",
    "| **Precision**            | $\\frac{TP}{TP + FP}$                                                | $[0, 1]$      | Of predicted positives, how many are correct.                  |\n",
    "| **Recall (Sensitivity)** | $\\frac{TP}{TP + FN}$                                                | $[0, 1]$      | Of actual positives, how many are correctly identified.        |\n",
    "| **F1-Score**             | $2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$         | $[0, 1]$      | Harmonic mean of precision and recall.                         |\n",
    "| **Specificity**          | $\\frac{TN}{TN + FP}$                                                | $[0, 1]$      | Ability to correctly identify negatives.                       |\n",
    "| **ROC-AUC**              | Area under ROC curve                                                | $[0, 1]$      | Higher values indicate better discrimination between classes.  |\n",
    "| **Log Loss**             | $-\\frac{1}{n} \\sum [ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) ]$ | $[0, \\infty)$ | Measures accuracy of probability predictions; lower is better. |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Confusion Matrix (for Classification)**\n",
    "\n",
    "|                     | Predicted Positive  | Predicted Negative  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Special Metrics (for Imbalanced Data)**\n",
    "\n",
    "* **Precision-Recall Curve:** Focuses on performance with imbalanced classes.\n",
    "* **FÎ²-Score:** Weighted F-score giving more importance to either precision or recall.\n",
    "* **Matthews Correlation Coefficient (MCC):** Balanced metric even for skewed datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Interview Insights**\n",
    "\n",
    "### âœ… **Basic Level**\n",
    "\n",
    "| Question                                          | Answer                                                                |\n",
    "| ------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| What metric do you use for regression?            | Common metrics: MSE, RMSE, MAE, $R^2$.                                |\n",
    "| What metric do you use for binary classification? | Accuracy, Precision, Recall, F1-Score, ROC-AUC.                       |\n",
    "| What is a confusion matrix?                       | A table showing correct and incorrect predictions for classification. |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Intermediate Level**\n",
    "\n",
    "| Question                                      | Answer                                                                                    |\n",
    "| --------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| Why is accuracy not always a good metric?     | In imbalanced datasets, accuracy can be misleading because it ignores class distribution. |\n",
    "| When would you prefer F1-score over accuracy? | When false positives and false negatives are equally important and data is imbalanced.    |\n",
    "| What does ROC-AUC measure?                    | The ability of a model to distinguish between classes at different thresholds.            |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Advanced Level**\n",
    "\n",
    "| Question                                            | Answer                                                                                                                             |\n",
    "| --------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Whatâ€™s the difference between precision and recall? | Precision focuses on correctness of positive predictions, while recall measures coverage of actual positives.                      |\n",
    "| Why use adjusted $R^2$ instead of $R^2$?            | Adjusted $R^2$ accounts for number of predictors, preventing artificial inflation.                                                 |\n",
    "| How do you choose a metric for a business problem?  | Based on the cost of misclassification errors and project goals (e.g., recall in medical diagnosis, precision in fraud detection). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2db5b",
   "metadata": {},
   "source": [
    "## ðŸ“˜ **Theory: Overfitting and Underfitting in Machine Learning**\n",
    "\n",
    "**Definition:**\n",
    "Overfitting and underfitting are two common problems in model training that affect a modelâ€™s ability to generalize to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Overfitting**\n",
    "\n",
    "| Aspect         | Description                                                                                                            |\n",
    "| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition** | When a model learns the training data too well, including noise and outliers, leading to poor performance on new data. |\n",
    "| **Cause**      | Model is too complex (too many parameters or features).                                                                |\n",
    "| **Symptoms**   | High training accuracy, low test accuracy.                                                                             |\n",
    "| **Example**    | A decision tree grown without pruning that memorizes training data.                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Underfitting**\n",
    "\n",
    "| Aspect         | Description                                                                                                                          |\n",
    "| -------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Definition** | When a model is too simple to capture underlying patterns in the data, resulting in poor performance on both training and test data. |\n",
    "| **Cause**      | Model lacks complexity or is improperly trained.                                                                                     |\n",
    "| **Symptoms**   | Low training accuracy and low test accuracy.                                                                                         |\n",
    "| **Example**    | Using a linear model to fit non-linear data.                                                                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Bias-Variance Trade-off**\n",
    "\n",
    "| Term         | Description                                                                          |\n",
    "| ------------ | ------------------------------------------------------------------------------------ |\n",
    "| **Bias**     | Error due to overly simplistic assumptions (underfitting).                           |\n",
    "| **Variance** | Error due to model sensitivity to small fluctuations in training data (overfitting). |\n",
    "| **Goal**     | Find a balance where both bias and variance are minimized.                           |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Techniques to Handle Overfitting**\n",
    "\n",
    "| Technique                 | Description                                             |\n",
    "| ------------------------- | ------------------------------------------------------- |\n",
    "| Cross-Validation          | Use validation data to tune model parameters.           |\n",
    "| Regularization (L1/L2)    | Penalize large coefficients to simplify the model.      |\n",
    "| Pruning (for trees)       | Limit depth or remove unnecessary branches.             |\n",
    "| Early Stopping            | Stop training before the model starts memorizing noise. |\n",
    "| Dropout (for neural nets) | Randomly drop neurons during training.                  |\n",
    "| Reduce Features           | Remove irrelevant or highly correlated features.        |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Techniques to Handle Underfitting**\n",
    "\n",
    "| Technique             | Description                                                  |\n",
    "| --------------------- | ------------------------------------------------------------ |\n",
    "| Add Features          | Include more informative predictors.                         |\n",
    "| Use Complex Models    | Choose models with higher capacity (e.g., ensemble methods). |\n",
    "| Reduce Regularization | Loosen constraints on parameters.                            |\n",
    "| Train Longer          | Allow model to learn more patterns from data.                |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Interview Insights**\n",
    "\n",
    "### âœ… **Basic Level**\n",
    "\n",
    "| Question                       | Answer                                                                         |\n",
    "| ------------------------------ | ------------------------------------------------------------------------------ |\n",
    "| What is overfitting?           | Itâ€™s when a model memorizes training data and fails to generalize to new data. |\n",
    "| What is underfitting?          | Itâ€™s when a model is too simple and fails to capture data patterns.            |\n",
    "| How do you detect overfitting? | Compare training and validation accuracy; large gap indicates overfitting.     |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Intermediate Level**\n",
    "\n",
    "| Question                                    | Answer                                                                                                           |\n",
    "| ------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| Explain the bias-variance trade-off.        | High bias leads to underfitting; high variance leads to overfitting. The trade-off seeks optimal generalization. |\n",
    "| What techniques prevent overfitting?        | Cross-validation, regularization, pruning, dropout, early stopping.                                              |\n",
    "| How does regularization reduce overfitting? | It adds a penalty term to the cost function to prevent large coefficients and model complexity.                  |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Advanced Level**\n",
    "\n",
    "| Question                                                                        | Answer                                                                                                                      |\n",
    "| ------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Why does adding more features sometimes lead to overfitting?                    | Because the model becomes more complex, capturing noise along with patterns.                                                |\n",
    "| How would you handle overfitting in a neural network?                           | Use dropout, early stopping, and data augmentation.                                                                         |\n",
    "| Can you explain a scenario where both overfitting and underfitting might occur? | When a model starts underfitting with insufficient training and then overfits as training continues without regularization. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124dd69",
   "metadata": {},
   "source": [
    "## ðŸ“˜ **Theory: Polynomial Linear Regression**\n",
    "\n",
    "**Definition:**\n",
    "Polynomial Linear Regression is an extension of simple and multiple linear regression where the relationship between the independent variable(s) and the dependent variable is modeled as an **nth-degree polynomial**.\n",
    "\n",
    "* It is still a **linear model** because coefficients are linear, but the features are transformed into polynomial terms.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Model Equation**\n",
    "\n",
    "For a single variable $x$ and polynomial degree $n$:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_n x^n + \\epsilon\n",
    "$$\n",
    "\n",
    "| Term                   | Meaning                             |\n",
    "| ---------------------- | ----------------------------------- |\n",
    "| $y$                    | Dependent (target) variable         |\n",
    "| $x$                    | Independent variable                |\n",
    "| $x^2, x^3, \\dots, x^n$ | Polynomial features (powers of $x$) |\n",
    "| $\\beta_i$              | Coefficients for each term          |\n",
    "| $\\epsilon$             | Error term                          |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **How It Works**\n",
    "\n",
    "| Step | Description                                                                   |\n",
    "| ---- | ----------------------------------------------------------------------------- |\n",
    "| 1    | Transform original feature(s) into polynomial features (e.g., $x^2, x^3$).    |\n",
    "| 2    | Apply linear regression on transformed features.                              |\n",
    "| 3    | Fit a curve (instead of a straight line) to capture non-linear relationships. |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                       | Disadvantages                                   |\n",
    "| ------------------------------------------------ | ----------------------------------------------- |\n",
    "| Captures non-linear patterns easily              | High-degree polynomials may lead to overfitting |\n",
    "| Simple to implement with linear regression tools | Sensitive to outliers                           |\n",
    "| Provides flexibility in curve fitting            | Computationally expensive for high-degree terms |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Overfitting Risk**\n",
    "\n",
    "* Higher polynomial degrees can create a curve that fits the training data very closely (overfitting).\n",
    "* Regularization (Ridge, Lasso) can help control complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Use Cases**\n",
    "\n",
    "* Modeling growth curves (population, sales trends)\n",
    "* Capturing non-linear trends in time series\n",
    "* Engineering applications where relationships are polynomial\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Interview Insights**\n",
    "\n",
    "### âœ… **Basic Level**\n",
    "\n",
    "| Question                                       | Answer                                                                                                      |\n",
    "| ---------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| What is polynomial regression?                 | Itâ€™s a regression technique where the model fits a polynomial equation to capture non-linear relationships. |\n",
    "| Is polynomial regression linear or non-linear? | Itâ€™s a linear model because itâ€™s linear in terms of coefficients $\\beta$, though features are polynomial.   |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Intermediate Level**\n",
    "\n",
    "| Question                                                | Answer                                                                                                                       |\n",
    "| ------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| How do you implement polynomial regression in practice? | Transform features using polynomial terms (e.g., via `PolynomialFeatures` in scikit-learn) and then apply linear regression. |\n",
    "| What happens when you increase polynomial degree?       | Model flexibility increases, but risk of overfitting also rises.                                                             |\n",
    "| How can you choose the right degree of the polynomial?  | Use cross-validation to determine the optimal complexity.                                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Advanced Level**\n",
    "\n",
    "| Question                                                                                     | Answer                                                                                                                                  |\n",
    "| -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Why might polynomial regression perform poorly on extrapolation?                             | Because high-degree polynomials can produce extreme values outside the training range.                                                  |\n",
    "| How do regularization methods (Ridge/Lasso) help polynomial regression?                      | They penalize large coefficients, reducing overfitting while still modeling non-linearity.                                              |\n",
    "| How is polynomial regression different from using non-linear algorithms like decision trees? | Polynomial regression assumes a parametric polynomial relationship, while decision trees capture non-linearity in a non-parametric way. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5d207",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
