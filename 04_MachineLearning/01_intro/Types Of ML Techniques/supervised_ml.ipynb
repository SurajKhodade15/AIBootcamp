{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de9de5b",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Supervised Learning is a machine learning paradigm in which the model is trained on a labeled dataset, meaning each input is paired with the correct output. The goal is to learn a function that maps inputs to desired outputs by minimizing prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da533b",
   "metadata": {},
   "source": [
    "### Categories of Supervised Learning\n",
    " ### üî∑ A. Regression Algorithms\n",
    " | **Algorithm**                      | **Type**             | **Use Case**                                 | **Key Evaluation Metrics** |\n",
    "| ---------------------------------- | -------------------- | -------------------------------------------- | -------------------------- |\n",
    "| **Linear Regression**              | Linear, Parametric   | Predict house price, sales forecasting       | RMSE, MAE, R¬≤              |\n",
    "| **Ridge Regression**               | Linear + L2 Regular. | Prevent overfitting in high-dimensional data | RMSE, MAE, R¬≤              |\n",
    "| **Lasso Regression**               | Linear + L1 Regular. | Feature selection, sparse models             | RMSE, MAE, R¬≤              |\n",
    "| **Elastic Net**                    | Linear + L1 + L2     | Combines Ridge and Lasso strengths           | RMSE, MAE, R¬≤              |\n",
    "| **Polynomial Regression**          | Non-linear           | Curve fitting, nonlinear trends              | RMSE, MAE, R¬≤              |\n",
    "| **Support Vector Regressor (SVR)** | Non-linear           | Predict stock prices, complex patterns       | RMSE, MAE, R¬≤              |\n",
    "| **Decision Tree Regressor**        | Non-parametric       | Predict demand/supply                        | RMSE, MAE                  |\n",
    "| **Random Forest Regressor**        | Ensemble             | Robust regression across diverse inputs      | RMSE, MAE                  |\n",
    "| **Gradient Boosting Regressor**    | Ensemble             | Predict performance scores                   | RMSE, MAE                  |\n",
    "| **XGBoost/LightGBM/ CatBoost**     | Boosting Ensemble    | Industry-grade high-performance models       | RMSE, MAE, R¬≤              |\n",
    "\n",
    "###  üî∑ B. Classification Algorithms\n",
    "   | **Algorithm**                             | **Type**          | **Use Case**                              | **Key Evaluation Metrics**           |\n",
    "| ----------------------------------------- | ----------------- | ----------------------------------------- | ------------------------------------ |\n",
    "| **Logistic Regression**                   | Linear, Binary    | Spam detection, credit approval           | Accuracy, Precision, Recall, AUC-ROC |\n",
    "| **Multinomial Logistic Regression**       | Multi-class       | Digit recognition, sentiment analysis     | F1-score, Log Loss                   |\n",
    "| **K-Nearest Neighbors (KNN)**             | Instance-based    | Image recognition, recommendation systems | Accuracy, Confusion Matrix           |\n",
    "| **Support Vector Classifier (SVC)**       | Margin-based      | Face detection, text categorization       | AUC-ROC, Precision, Recall           |\n",
    "| **Decision Tree Classifier**              | Tree-based        | Churn prediction, fraud detection         | Accuracy, Gini/Entropy, F1-score     |\n",
    "| **Random Forest Classifier**              | Ensemble          | Medical diagnosis, bank loan approvals    | AUC-ROC, Accuracy                    |\n",
    "| **Gradient Boosting Classifier**          | Ensemble          | Insurance claim prediction                | AUC, LogLoss                         |\n",
    "| **XGBoost / LightGBM / CatBoost**         | Gradient Boosting | High-performance real-time classification | AUC, F1-score                        |\n",
    "| **Naive Bayes**                           | Probabilistic     | Sentiment analysis, spam classification   | Accuracy, Precision, Log Loss        |\n",
    "| **Quadratic Discriminant Analysis (QDA)** | Statistical       | Pattern recognition, facial recognition   | Accuracy                             |\n",
    "\n",
    "### üß† When to Use Which Technique?\n",
    "| **Scenario**                     | **Recommended Model(s)**                             |\n",
    "| -------------------------------- | ---------------------------------------------------- |\n",
    "| High-dimensional data            | Lasso, Ridge, Elastic Net                            |\n",
    "| Complex non-linear relationships | Polynomial, SVM, Gradient Boosting                   |\n",
    "| Real-time prediction needs       | XGBoost, LightGBM                                    |\n",
    "| Imbalanced classes               | Random Forest with class weighting, SMOTE + Logistic |\n",
    "| Small dataset                    | KNN, Decision Tree                                   |\n",
    "| Feature selection needed         | Lasso Regression, Tree-based models                  |\n",
    "\n",
    "### üìå Model Selection Criteria\n",
    "Speed: Logistic Regression, Naive Bayes\n",
    "\n",
    "Accuracy: Random Forest, XGBoost\n",
    "\n",
    "Interpretability: Logistic Regression, Decision Trees\n",
    "\n",
    "Scalability: LightGBM, CatBoost\n",
    "\n",
    "Explainability: SHAP values with Tree-based models\n",
    "\n",
    "### Evaluation Metrics for Supervised Learning\n",
    "| Metric                | Description                                                                 |\n",
    "| --------------------- | --------------------------------------------------------------------------- |     \n",
    "| **Accuracy**           | The proportion of correct predictions out of total predictions.            |\n",
    "| **Precision**          | The proportion of true positive predictions out of all positive predictions. |\n",
    "| **Recall (Sensitivity)** | The proportion of true positive predictions out of all actual positive instances. |\n",
    "| **F1 Score**           | The harmonic mean of precision and recall, balancing both metrics\n",
    "| **ROC-AUC**           | The area under the Receiver Operating Characteristic curve, measuring the trade-off between true positive rate and false positive rate. |\n",
    "\n",
    "### Challenges in Supervised Learning\n",
    "| Challenge            | Description                                                                 |\n",
    "| ------------------- | --------------------------------------------------------------------------- |\n",
    "| **Overfitting**      | When the model learns noise in the training data, leading to poor generalization on unseen data. |\n",
    "| **Underfitting**     | When the model is too simple to capture the underlying patterns in the data. |\n",
    "| **Imbalanced Data**  | When one class is significantly more frequent than others, leading to biased predictions. |    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39251e",
   "metadata": {},
   "source": [
    "**Q1. What is Supervised Learning?**  \n",
    "Supervised learning is a subset of machine learning where the model is trained on a labeled dataset. This means each training sample has an associated target or output label. The objective is for the model to learn a mapping from inputs to outputs that can generalize well to unseen data.\n",
    "\n",
    "**Q2. What are the main categories under Supervised Learning?**  \n",
    "Supervised Learning is primarily divided into:\n",
    "\n",
    "- Regression ‚Äì predicting continuous numeric values (e.g., house price prediction).\n",
    "- Classification ‚Äì predicting categorical outcomes (e.g., spam detection, fraud classification).\n",
    "\n",
    "**Q3. Name some widely used supervised learning algorithms.**  \n",
    "Key supervised learning techniques include:\n",
    "\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Naive Bayes\n",
    "- Gradient Boosting Machines (GBM), XGBoost, LightGBM\n",
    "- Neural Networks (when used with labeled data)\n",
    "\n",
    "**Q4. What is the difference between Linear Regression and Logistic Regression?**  \n",
    "Linear Regression is used for predicting continuous outcomes and assumes a linear relationship between the dependent and independent variables.  \n",
    "Logistic Regression, on the other hand, is used for binary or multi-class classification problems and outputs probabilities using a sigmoid or softmax function.\n",
    "\n",
    "**Q5. What are the assumptions of Linear Regression?**\n",
    "\n",
    "- Linearity: Relationship between input and output must be linear.\n",
    "- Homoscedasticity: Constant variance of errors.\n",
    "- Independence: Observations should be independent.\n",
    "- Normality: Residuals should be normally distributed.\n",
    "- No multicollinearity: Features should not be highly correlated.\n",
    "\n",
    "**Q6. What are the advantages and disadvantages of Decision Trees?**  \n",
    "Advantages:\n",
    "\n",
    "- Easy to interpret and visualize\n",
    "- Handles both numerical and categorical data\n",
    "- Requires minimal data preprocessing\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Prone to overfitting\n",
    "- Can be unstable due to slight variations in data\n",
    "- Biased towards features with more levels\n",
    "\n",
    "**Q7. How does Random Forest overcome the limitations of a Decision Tree?**  \n",
    "Random Forest is an ensemble technique that builds multiple decision trees on bootstrapped subsets of data and averages their predictions (for regression) or uses majority voting (for classification). This reduces variance and overfitting while improving model robustness and generalization.\n",
    "\n",
    "**Q8. Explain the working of K-Nearest Neighbors (KNN).**  \n",
    "KNN is a non-parametric, instance-based algorithm that assigns a label based on the majority class among the 'K' closest data points in the feature space. The closeness is typically measured using Euclidean or Manhattan distance. KNN works well for small datasets but can be computationally expensive on large-scale data.\n",
    "\n",
    "**Q9. When would you prefer using Support Vector Machines (SVM)?**  \n",
    "SVMs are preferred when the data is high-dimensional and requires a clear margin of separation. They work well in binary classification problems and are robust to overfitting, especially when using a proper kernel function (e.g., linear, polynomial, RBF). However, they are less effective for very large datasets.\n",
    "\n",
    "**Q10. What is Naive Bayes and when is it useful?**  \n",
    "Naive Bayes is a probabilistic classifier based on Bayes‚Äô Theorem with an assumption that features are independent given the class label. It is highly efficient and effective for text classification problems like spam detection and sentiment analysis. Despite its naive assumptions, it performs surprisingly well in many real-world applications.\n",
    "\n",
    "**Q11. What is the significance of hyperparameters in supervised models?**  \n",
    "Hyperparameters control the training behavior of models. For instance:\n",
    "\n",
    "- In Random Forest: number of trees (n_estimators), tree depth (max_depth)\n",
    "- In SVM: regularization parameter C, kernel type\n",
    "- In KNN: value of K\n",
    "\n",
    "Proper tuning of these using techniques like Grid Search or Random Search can significantly enhance model performance.\n",
    "\n",
    "**Q12. How do you evaluate the performance of classification models?**  \n",
    "Common evaluation metrics include:\n",
    "\n",
    "- Accuracy: Correct predictions over total predictions\n",
    "- Precision: Correct positive predictions over total predicted positives\n",
    "- Recall: Correct positive predictions over total actual positives\n",
    "- F1-Score: Harmonic mean of Precision and Recall\n",
    "- ROC-AUC: Area under the ROC Curve, indicating separability between classes\n",
    "\n",
    "**Q13. How do you handle class imbalance in supervised classification tasks?**  \n",
    "You can handle class imbalance using:\n",
    "\n",
    "- Resampling (Oversampling minority, Undersampling majority)\n",
    "- SMOTE (Synthetic Minority Oversampling Technique)\n",
    "- Adjusting class weights in algorithms\n",
    "- Choosing appropriate metrics like F1-Score or ROC-AUC instead of Accuracy\n",
    "\n",
    "**Q14. What is Gradient Boosting? How is it different from Bagging?**  \n",
    "Gradient Boosting builds models sequentially, where each new model tries to correct the residuals (errors) of the previous model.  \n",
    "Bagging, on the other hand, builds models in parallel using bootstrapped data and aggregates their outputs. Random Forest is a classic bagging approach, whereas XGBoost is a form of gradient boosting.\n",
    "\n",
    "**Q15. Explain the difference between L1 and L2 regularization.**\n",
    "\n",
    "- L1 (Lasso Regression): Encourages sparsity, driving some coefficients to zero, thereby performing feature selection.\n",
    "- L2 (Ridge Regression): Penalizes large coefficients but doesn‚Äôt eliminate features; good for multicollinearity.\n",
    "\n",
    "Elastic Net combines both L1 and L2 penalties for balanced regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46364b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4747860c",
   "metadata": {},
   "source": [
    "**Q1. What is Polynomial Regression, and when is it used?**  d?**  \n",
    "Polynomial Regression is an extension of Linear Regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial. It is used when data shows a non-linear trend but can still be modeled using polynomial terms of the input features.    \n",
    "However, excessive polynomial degrees can lead to overfitting, so regularization is recommended.\n",
    "\n",
    "**Q2. What is Ridge Regression?**  n?**  \n",
    "Ridge Regression is a regularized version of Linear Regression that adds an L2 penalty (squared magnitude of coefficients) to the loss function.    \n",
    "This helps in:\n",
    "- Reducing model complexity- Reducing model complexity\n",
    "- Handling multicollinearityity\n",
    "- Preventing overfitting- Preventing overfitting\n",
    "\n",
    "The formula becomes:  The formula becomes:  \n",
    "Loss = RSS + Œ± * ‚àë(Œ∏¬≤)    \n",
    "Where Œ± is the regularization parameter.arization parameter.\n",
    "\n",
    "**Q3. What is Lasso Regression? How does it differ from Ridge Regression?**   it differ from Ridge Regression?**  \n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regression adds an L1 penalty (absolute value of coefficients) to the loss function.  Lasso (Least Absolute Shrinkage and Selection Operator) Regression adds an L1 penalty (absolute value of coefficients) to the loss function.  \n",
    "It not only reduces overfitting but also performs feature selection by shrinking some coefficients to zero.shrinking some coefficients to zero.\n",
    "\n",
    "Difference:\n",
    "- Ridge shrinks coefficients but retains all features- Ridge shrinks coefficients but retains all features\n",
    "- Lasso can eliminate irrelevant features (sparse solution) eliminate irrelevant features (sparse solution)\n",
    "\n",
    "**Q4. What is Elastic Net Regression?**  \n",
    "Elastic Net combines both L1 and L2 regularization. It is useful when:Elastic Net combines both L1 and L2 regularization. It is useful when:\n",
    "- You have many features\n",
    "- You expect correlated features- You expect correlated features\n",
    "- You want a balance between Ridge (stability) and Lasso (sparsity)(stability) and Lasso (sparsity)\n",
    "\n",
    "Formula:  Formula:  \n",
    "Loss = RSS + Œ±‚ÇÅ * ‚àë|Œ∏| + Œ±‚ÇÇ * ‚àë(Œ∏¬≤) + Œ±‚ÇÇ * ‚àë(Œ∏¬≤)\n",
    "\n",
    "**Q5. What is Logistic Regression? How is it trained?**  ion? How is it trained?**  \n",
    "Logistic Regression is used for binary or multiclass classification. It models the probability that a given input belongs to a certain class using the sigmoid function:Logistic Regression is used for binary or multiclass classification. It models the probability that a given input belongs to a certain class using the sigmoid function:\n",
    "\n",
    "P(y=1|x) = 1 / (1 + e^(-z)) where z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚ÇôP(y=1|x) = 1 / (1 + e^(-z)) where z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô\n",
    "\n",
    "It is trained using Maximum Likelihood Estimation (MLE) rather than least squares, aiming to maximize the probability of correctly classified observations.ood Estimation (MLE) rather than least squares, aiming to maximize the probability of correctly classified observations.\n",
    "\n",
    "**Q6. What are the strengths and weaknesses of Support Vector Machines (SVM)?**  ort Vector Machines (SVM)?**  \n",
    "Strengths:\n",
    "- Effective in high-dimensional spaces- Effective in high-dimensional spaces\n",
    "- Robust to overfitting in low-noise datasets\n",
    "- Kernel trick enables non-linear decision boundaries- Kernel trick enables non-linear decision boundaries\n",
    "\n",
    "Weaknesses:Weaknesses:\n",
    "- Poor scalability to large datasets\n",
    "- Performance is sensitive to hyperparameters (C, Œ≥)nce is sensitive to hyperparameters (C, Œ≥)\n",
    "- Less interpretable than decision trees- Less interpretable than decision trees\n",
    "\n",
    "**Q7. Explain the kernel trick in SVM.**  **Q7. Explain the kernel trick in SVM.**  \n",
    "The kernel trick allows SVM to operate in a high-dimensional, implicit feature space without explicitly computing the coordinates.   high-dimensional, implicit feature space without explicitly computing the coordinates.  \n",
    "Common kernels:Common kernels:\n",
    "- Linear Kernel\n",
    "- Polynomial Kernel- Polynomial Kernel\n",
    "- Radial Basis Function (RBF)sis Function (RBF)\n",
    "- Sigmoid Kernel- Sigmoid Kernel\n",
    "\n",
    "It is particularly useful when data is not linearly separable in the original feature space.It is particularly useful when data is not linearly separable in the original feature space.\n",
    "\n",
    "**Q8. What is Gradient Boosting and how does it work?**  **Q8. What is Gradient Boosting and how does it work?**  \n",
    "Gradient Boosting builds an ensemble of weak learners (usually decision trees) sequentially. Each new learner is trained to correct the residual errors of the combined previous learners.f weak learners (usually decision trees) sequentially. Each new learner is trained to correct the residual errors of the combined previous learners.\n",
    "\n",
    "Process:\n",
    "- Initialize a base prediction (e.g., mean of target)\n",
    "- Compute residuals (errors)uals (errors)\n",
    "- Fit a weak learner on residuals- Fit a weak learner on residuals\n",
    "- Add learner to the ensemble to the ensemble\n",
    "- Repeat for N iterations- Repeat for N iterations\n",
    "\n",
    "Popular variants:Popular variants:\n",
    "- XGBoost: Regularized, fast, parallelizedt, parallelized\n",
    "- LightGBM: Uses histogram-based tree growth- LightGBM: Uses histogram-based tree growth\n",
    "- CatBoost: Handles categorical variables nativelyndles categorical variables natively\n",
    "\n",
    "**Q9. What is the difference between Bagging and Boosting?**  \n",
    "Bagging (Bootstrap Aggregating):Bagging (Bootstrap Aggregating):\n",
    "- Learners are built in parallel\n",
    "- Reduces variance\n",
    "- Example: Random Forest- Example: Random Forest\n",
    "\n",
    "Boosting:Boosting:\n",
    "- Learners are built sequentially\n",
    "- Reduces bias- Reduces bias\n",
    "- Example: Gradient Boosting, XGBoostng, XGBoost\n",
    "\n",
    "**Q10. What is the purpose of the confusion matrix in classification tasks?**  he confusion matrix in classification tasks?**  \n",
    "The confusion matrix is a performance evaluation metric that provides insight into:The confusion matrix is a performance evaluation metric that provides insight into:\n",
    "- True Positives (TP)\n",
    "- False Positives (FP)- False Positives (FP)\n",
    "- True Negatives (TN)\n",
    "- False Negatives (FN)- False Negatives (FN)\n",
    "\n",
    "From it, we can derive key metrics:From it, we can derive key metrics:\n",
    "- Accuracy = (TP + TN) / Total\n",
    "- Precision = TP / (TP + FP)- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN)\n",
    "- F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)- F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "\n",
    "**Q11. What is Cross-Validation, and why is it used?**  **Q11. What is Cross-Validation, and why is it used?**  \n",
    "Cross-validation is a technique to evaluate model generalization by splitting the dataset into training and validation folds multiple times.  lization by splitting the dataset into training and validation folds multiple times.  \n",
    "The most common method is K-Fold Cross Validation, which: Cross Validation, which:\n",
    "- Reduces variance in model evaluation- Reduces variance in model evaluation\n",
    "- Ensures every data point is used for both training and validationused for both training and validation\n",
    "\n",
    "**Q12. What is early stopping in boosting models?**  arly stopping in boosting models?**  \n",
    "Early stopping halts training if the model performance on a validation set does not improve for a specified number of iterations.  Early stopping halts training if the model performance on a validation set does not improve for a specified number of iterations.  \n",
    "It is used to prevent overfitting in iterative algorithms like XGBoost or Neural Networks.overfitting in iterative algorithms like XGBoost or Neural Networks.\n",
    "\n",
    "**Q13. How do you handle multicollinearity in regression models?**  w do you handle multicollinearity in regression models?**  \n",
    "Multicollinearity occurs when independent variables are highly correlated. To address it:Multicollinearity occurs when independent variables are highly correlated. To address it:\n",
    "- Use Ridge or Elastic Net regularizationlarization\n",
    "- Perform Principal Component Analysis (PCA)- Perform Principal Component Analysis (PCA)\n",
    "- Drop one of the correlated featuresf the correlated features\n",
    "- Analyze Variance Inflation Factor (VIF) and remove high-VIF features- Analyze Variance Inflation Factor (VIF) and remove high-VIF features\n",
    "\n",
    "**Q14. How do tree-based models handle missing data?**  **Q14. How do tree-based models handle missing data?**  \n",
    "Tree-based models like XGBoost and LightGBM can:\n",
    "- Learn optimal split direction for missing values\n",
    "- Use surrogate splits (alternate feature splits when values are missing)- Use surrogate splits (alternate feature splits when values are missing)\n",
    "- Impute missing values internally during traininglues internally during training\n",
    "\n",
    "**Q15. When should you choose Logistic Regression over complex models like XGBoost?**  ou choose Logistic Regression over complex models like XGBoost?**  \n",
    "Use Logistic Regression when:Use Logistic Regression when:\n",
    "- Interpretability is a priority (e.g., healthcare, finance)is a priority (e.g., healthcare, finance)\n",
    "- The dataset is small or linearly separable- The dataset is small or linearly separable\n",
    "- Real-time prediction speed is criticalon speed is critical\n",
    "- There is limited feature interaction and non-linearity- There is limited feature interaction and non-linearity\n",
    "\n",
    "XGBoost is more suitable when you require:XGBoost is more suitable when you require:\n",
    "- High predictive accuracy\n",
    "- Complex decision boundaries- Complex decision boundaries\n",
    "- Robust handling of outliers and missing dataers and missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256de42c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
